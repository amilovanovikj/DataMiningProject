{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ElasticNetCV,LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import featuretools as ft\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import plot_importance\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y, yhat, title=\"Predictions vs Actual\", output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot the predictions against the actual values\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    y : actual (real) values\n",
    "\n",
    "    yhat : predicted values\n",
    "\n",
    "    title : plot title\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('PM10')\n",
    "    plt.plot(y, label=\"actual\", figure=fig)\n",
    "    plt.plot(yhat, label=\"predicted\", figure=fig)\n",
    "    plt.title(title)\n",
    "    fig.legend()\n",
    "\n",
    "    if output_dir != None:\n",
    "        plt.savefig(os.path.join(output_dir, \"{}.png\".format(title)))\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_and_evaluate(name, model, train,final_eval, output_dir, predictors):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model through the scikit-learn pipeline\n",
    "\n",
    "    A pipeline is build using the supplied model as the final step in the pipeline (see make_pipeline)\n",
    "    Then, the input data is transformed accordingly to be able to fit into the pipeline.\n",
    "\n",
    "    The model is saved after the last step under `output_dir`/models with the filename `name`.joblib,\n",
    "    if you wish to load them later and reevaluate/retrain them.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    name : the name of the model\n",
    "\n",
    "    model : a scikit-learn model object\n",
    "\n",
    "    train : train portion of the dataset\n",
    "\n",
    "    final_eval : last 2*X:X hours from the dataset\n",
    "\n",
    "    output_dir : base directory for saving output files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : mean absolute error of the trained model on last X hours\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---\" * 5)\n",
    "    print(\"Running pipeline for {}\".format(name))\n",
    "\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\n",
    "\n",
    "    pipeline = model\n",
    "\n",
    "    X_train, y_train = train.drop(\n",
    "        [\"PM10\"], axis=1)[predictors].values, train[\"PM10\"].values\n",
    "    X_final, y_final = final_eval.drop(\n",
    "        [\"PM10\"], axis=1)[predictors].values, final_eval[\"PM10\"].values\n",
    "    print(\"Fitting pipeline on all \\\"all available data\\\"\")\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    yhat = pipeline.predict(X_final)\n",
    "    yhat_return = np.expm1(yhat)\n",
    "    y_final_return = np.expm1(y_final)\n",
    "    mae = mean_absolute_error(y_final_return, yhat_return)\n",
    "    print(\"MAE: {}\".format(mae))\n",
    "    plot_predictions(y_final_return, yhat_return, title=\"{} - Predicted vs. Actual\".format(name), output_dir=plot_dir)\n",
    "\n",
    "    # save the model\n",
    "    joblib.dump(model, os.path.join(\n",
    "        output_dir, \"models\", \"{}.joblib\".format(name)))\n",
    "\n",
    "    return yhat_return, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple stacking\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "    \n",
    "averaged_models = AveragingModels(models = (ENet, GBoost, lasso,model_xgb,model_lgb))\n",
    "\n",
    "score = mae(averaged_models,predictors)\n",
    "print(\" Averaged base models score: {:.4f} \\n\",score)\n",
    "\n",
    "\n",
    "# STACKING WITH A META MODEL\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_series(path, ts_column=\"time\"):\n",
    "    \"\"\"\n",
    "    Read a time series from a CSV file.\n",
    "\n",
    "    The CSV file must contain a column with either a UNIX timestamp or a datetime\n",
    "    string with any format supported by Pandas. \n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    path : path to CSV file\n",
    "\n",
    "    ts_column : name of the column containing time data, \"time\" by default\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : Pandas Series object with datetime as index\n",
    "    \"\"\"\n",
    "\n",
    "    # read CSV\n",
    "    df = pd.read_csv(path, parse_dates=[ts_column])\n",
    "    # convert timestamps to datetime objects using panda's to_datetime\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column], unit=\"s\")\n",
    "    # set datetime as index (make time series)\n",
    "    df.index = df[ts_column]\n",
    "    # delete original time column\n",
    "    del df[ts_column]\n",
    "\n",
    "    # remove rows with duplicated time if there are any, keep first duplicate row\n",
    "    df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    df.index.name = ts_column\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_directory_tree(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Create the output directory tree structure specified by `tree` in `output_dir`\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    tree : list of paths to create under `output_dir`\n",
    "\n",
    "    output_dir : path to root of output directory tree\n",
    "    \"\"\"\n",
    "\n",
    "    for d in tree:\n",
    "        try:\n",
    "            path = os.path.join(output_dir, d)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                print(\"Path already exists: {}\".format(d))\n",
    "                print(\"Files may be overwritten\")\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def get_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Get all .csv filenames from the specified directory\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    data_dir : path to directory containing .csv files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : list containing dataset filenames\n",
    "    \"\"\"\n",
    "\n",
    "    return [f for f in os.listdir(data_dir) if os.path.isfile(\n",
    "        os.path.join(data_dir, f)) and f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "def run(input_dir, output_dir, models, model_names, team_name=\"OrganizersTeam\", predict_window=8760):\n",
    "    \"\"\"\n",
    "    Train and evaluate models for each dataset under `input_dir`\n",
    "\n",
    "    This script trains and evaluates 6 models and ensembles them using meta model stacking\n",
    "    1. Lasso\n",
    "    2. ENet\n",
    "    3. KernelRidge\n",
    "    4. GradientBoostingRegressor\n",
    "    5. XGB\n",
    "    6. LGB\n",
    "    7. Meta stacking of previously mentioned models, LGB acting as the meta\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    input_dir : directory containing datasets\n",
    "\n",
    "    output_dir : directory for saving useful output files (models, etc)\n",
    "\n",
    "    predict_window : number of hours needed to predict, default=12\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : a Pandas DataFrame containing the scores for each trained model\n",
    "    \"\"\"\n",
    "\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    sub_dir = os.path.join(output_dir, \"submissions\")\n",
    "    submission_file_name_fmt = \"{}_{}.csv\"\n",
    "\n",
    "    make_directory_tree([\"models\",\"plots\", \"submissions\"], output_dir)\n",
    "\n",
    "    datasets = get_datasets(input_dir)\n",
    "\n",
    "    # create a scores table to keep MAE for each location:model pair\n",
    "    scores = pd.DataFrame(columns=[\"Location\", \"Model\", \"MAE\"])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # load the dataset\n",
    "        df = read_csv_series(os.path.join(input_dir, dataset))\n",
    "        loc = dataset.split(\".\")[0]\n",
    "\n",
    "        # shift PM10 for `predict_window` hours ahead\n",
    "        df[\"PM10\"] = df[\"PM10\"].shift(-predict_window)\n",
    "\n",
    "        # split dataset into train, test and evaluation by dates\n",
    "        train_len = int((len(df) - (2 * predict_window)))\n",
    "        train= df[:train_len]\n",
    "        final_eval = df[-(2 * predict_window):-predict_window].copy()\n",
    "        \n",
    "        # initialize model, PUT OPTIMIZED MODELS HERE\n",
    "        print(\"Will train a total of {} models\".format(len(models) * 1))\n",
    "\n",
    "        mae_min = 1e10\n",
    "        yhat_sub = []\n",
    "        predictors = [x for x in df.columns if x not in [\"PM10\",\"time\"]]\n",
    "        \n",
    "        for model_name,model in zip(model_names,models):\n",
    "            # get predictions and MAE\n",
    "            yhat, mae = train_and_evaluate(\"{} - {}\".format(loc, model_name), model, train,\n",
    "                                           final_eval, output_dir, predictors)\n",
    "\n",
    "            # save the score (MAE) for the model\n",
    "            scores = scores.append(\n",
    "                {\"Location\": loc, \"Model\": model_name, \"MAE\": mae}, ignore_index=True)\n",
    "\n",
    "            # save the better predictions to `yhat_sub`\n",
    "            if mae < mae_min:\n",
    "                mae_min = mae\n",
    "                yhat_sub = yhat\n",
    "\n",
    "        sub_df = pd.DataFrame(yhat_sub, columns=[\"PM10\"])\n",
    "        sub_df.to_csv(os.path.join(\n",
    "            sub_dir, submission_file_name_fmt.format(team_name, loc)))\n",
    "\n",
    "        scores.to_csv(os.path.join(output_dir, \"scores.csv\"))\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(\"Saved models can be found at {}\".format(models_dir))\n",
    "    print(\"Plots can be found at {}\".format(plots_dir))\n",
    "    print(\"Submissions can be found at {}\".format(sub_dir))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = 'pm10_scores'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1d89ee546aa9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Testing lasso, enet, gboost, xgboost, lgboost and a stacking model with lgboost as meta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mlasso\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRobustScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLasso\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0menet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mRobustScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mElasticNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0005\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml1_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
      "\u001b[1;31mNameError\u001b[0m: name 'make_pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing lasso, enet, gboost, xgboost, lgboost and a stacking model with lgboost as meta\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                       max_depth=4, max_features='sqrt',\n",
    "                       min_samples_leaf=15, min_samples_split=10, \n",
    "                       loss='huber', random_state =5)\n",
    "xgboost = xgb.XGBRegressor(learning_rate =0.1, # OTPIMIZED, not to be changed\n",
    "                           n_estimators=1200,\n",
    "                           max_depth=2,\n",
    "                           min_child_weight=0.5,\n",
    "                           gamma=0,\n",
    "                           subsample=0.8,\n",
    "                           n_jobs=-1,\n",
    "                           colsample_bytree=0.8,\n",
    "                           reg_alpha=0.01,\n",
    "                           random_state=42)\n",
    "models=[lasso,enet,gboost,xgboost]\n",
    "\n",
    "lgboost = lgb.LGBMRegressor(objective='regression',num_leaves=5, # THE META MODEL\n",
    "                      learning_rate=0.05, n_estimators=720,\n",
    "                      max_bin = 55, bagging_fraction = 0.8,\n",
    "                      bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                      feature_fraction_seed=9, bagging_seed=9,\n",
    "                      min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "\n",
    "stacked_averaged_models = StackingAveragedModels(base_models = models,\n",
    "                                                 meta_model = lgboost)\n",
    "\n",
    "final_models = [lasso, enet, xgboost, lgboost, stacked_averaged_models]\n",
    "model_names = [\"lasso\",\"enet\",\"xgboost\",\"lgboost\",\"stacked_model_with_lgboost_meta\"]\n",
    "# input_folder = \"blabla\"\n",
    "# run(input_folder, output_dir, final_models, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing Ridge, SVM, decision regression tree, random forest, Multi-layer Perceptron regressor\n",
    "\n",
    "krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "svr = make_pipeline(StandardScaler(), SVR(C=1.0, epsilon=0.2))\n",
    "tree_regressor = DecisionTreeRegressor(random_state=0)\n",
    "forest_regressor = RandomForestRegressor(random_state=0)\n",
    "mlp_regr = MLPRegressor(random_state=1, max_iter=500)\n",
    "\n",
    "final_models = [krr, svr, tree_regressor, forest_regressor, mlp_regr]\n",
    "model_names = [\"kernel_ridge\", \"svr\", \"tree_regressor\", \"forest_regressor\", \"mlp_regressor\"]\n",
    "# input_folder = \"blabla\"\n",
    "# run(input_folder, output_dir, final_models, model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing ARIMA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
