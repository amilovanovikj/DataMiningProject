- The notebooks folder contains notebooks for each separate subject concerning the project. The notebooks are split into 3 different subfolders:
Data preprocessing, EDA and Model evaluation.
- The Data preprocessing folder contains the notebooks:
	DataAggregationPipeline - notebook used as pipeline for aggregating and joining the weather and pollution datasets of all municipalities.
	This notebook is used to generate the combined(weather+pollution) datasets and also provides minimal required preprocessing steps. 
	The pre-processing of the pollution reports contains of: making the time column an hourly index of the dataframe; fixing typos in all columns (some
	values contain random dots and aren't read properly - we fix this to get more data); casting categorical data to numeric and leaving the 
	values which can't be cast as NaNs to deal with them in the PreprocessingPipeline notebook; removing duplicate times(rows with same index);
	removing rows with PM10>1000 or PM10<0 and interpolating up to maximum of 1 missing value of PM10 using a 'linear' interpolation method; 
	removing all rows with NaN values for the PM10 cell. 
	The preprocessing of the weather reports contains of: dropping the icon column; replacing the NaN values for precipType, precipAccumulation
	and cloudCover columns with their theoretical NaN values; interpolating the pressure columns up to maximum of 3 missing values; one-hot 
	encoding each categorical variable
	Finally the notebook combines both of these preprocessed reports into a single report. The notebook also has a section for an example procedure
	for the center municipality. We can use these section to test out more ideas for the data aggregation/joining procedure alongside the minimal
	preprocessing used.
	Feature engineering ideas and proposals and pipeline - notebook used for testing feature engineering ideas and proposals. The notebook also offers a pipeline
	where the proposed feature engineering can be ran on all the combined reports for each municipality. Ideas so far: lagging features; stat features like mean/median
	of certain time frames for imputing means; cov analysis. 
	PreprocessingPipeline - notebook for preprocessing of the combined datasets. The notebook offers a pipeline where every preprocessing step should be used. The notebook
	also has an example preprocessing on the center combined dataset which is also copied in the pipeline. This notebook can be used alongside the feature engineering notebook
	but should represent the final pipeline for generating the datasets which will be fed into models. Ideas and preprocessing on separate datasets can be done at the end of the 
	notebook or instead of the center example.
	This notebook contains the next preprocessing techniques so far: logarthiming the target variable because it is right skewed to enforce normal distribution; dropping ozone
	and windGust variables; dropping all rows where Foggy is nan to drop correlated rows with missing temperature values; correlation analysis; plots for outlier detection; 
	setting values of CO>300, NO2>250 and PM25>1000 to nans; interpolating each missing col with limit=2 and default interpolation; replacing windBearing with 0 if NaN;
	imputing pressure such that if the row is NaN we replace it with the median value of that hour of that month for each year(same for precipProb,precipInt,medianUvi,windSpeed);
	filling other nans of uvIndex with mean; dropping CO2,CO,SO2,NO2 since 70% of values are missing; dropping AQI to generate new one; imputing O3 and PM25 the same way as before
	and imputing left over nans with limit=3. Generating AQI features for PM25,PM10,O3 by getting the mean for the last 1 day of the corresponding features(looked online).
	Generating lag feature for PM10 which is mean of the last 3 hours; Generating delta features(current-prev) for cols:'O3', 'PM10', 'PM25', 'cloudCover', 'dewPoint', 'humidity',
       'precipAccumulation', 'precipIntensity', 'precipProbability','pressure', 'temperature', 'uvIndex', 'visibility', 'windBearing','windSpeed','AQI_PM25', 'AQI_PM10', 'AQI_O3','PM10_history'
	- this takes a very long time ~ 30mins;
	doing skew analysis on every numerical feature and then doing box cot transformation or log transformation on skewed features with skeweness>0.75
- The EDA folder should contain notebooks strictly for exploratory data analysis of the combined datasets and various plots/visualizations to gather preprocessing ideas
- The model evaluation folder contains the notebooks:
	ModelEvaluationPipeline - main notebook of the folder. Used for using already optimized(hyperparametar wise) models to use as regressive models to predict the PM10 value for the next x hours.
	Only the PM10 values are shifted up in the dataset for x. In this way, the interval [-2x,-x] of the dataset if used as evaluation set, the model will learn the relationship how to predict the next 
	x values(since the PM10 are shifted). The training set finally consists of the interval [0,-2x] and the final evaluation(test) set is [-2x,-x]. Each model's prediction is evaluated and the best one
	is chosen to generate the submission file in the sumbission folder. Refer to the Data description in the Data folder for more. 
	XGBoost training -> should be used to train and test new models on the combined preprocessed datasets. Contains grid search techniques for hyperparametar tuning