{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import matplotlib as matplot\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gzip\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import codecs\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_validate,GridSearchCV,train_test_split,RandomizedSearchCV\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Centar shape:  (37412, 42)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"centar_preprocessed.csv\")\n",
    "print(\"Centar shape: \",df.shape)\n",
    "df['PM10'] = np.expm1(df['PM10'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df['PM10']\n",
    "target = \"PM10\"\n",
    "train = df.drop(['PM10','time'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_evaluation_splits():\n",
    "    predict_window = 12\n",
    "    df_evaluation = df.copy()\n",
    "    df_evaluation[\"PM10\"] = df[\"PM10\"].shift(-predict_window)\n",
    "\n",
    "    train_len = int(len(df_evaluation) - 2 * predict_window)\n",
    "    train = df_evaluation[:train_len]\n",
    "    final_eval = df_evaluation[-(2 * predict_window):-predict_window]\n",
    "\n",
    "    X_train, y_train = train.drop(\n",
    "            [\"PM10\",\"time\"], axis=1), train[\"PM10\"]\n",
    "    X_final, y_final = final_eval.drop(\n",
    "        [\"PM10\",\"time\"], axis=1), final_eval[\"PM10\"]\n",
    "    \n",
    "    return X_train,y_train,X_final,y_final\n",
    "\n",
    "def evaluate_model(model,predictors):\n",
    "    X_train,y_train,X_final,y_final = get_evaluation_splits()\n",
    "    model.fit(X_train[predictors],y_train)\n",
    "    predictions = model.predict(X_final[predictors])\n",
    "    mae = mean_absolute_error(y_final, predictions)\n",
    "    print(\"MAE score for model: \",mae)\n",
    "    feat_imp = pd.Series(model.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "    feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "    plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n",
    "                       model, param_grid, cv=5, scoring_fit='neg_mean_absolute_error'):\n",
    "    gs = RandomizedSearchCV(\n",
    "        estimator=model,\n",
    "        param_distributions=param_grid, \n",
    "        cv=cv, \n",
    "        n_jobs=-1, \n",
    "        scoring=scoring_fit,\n",
    "        verbose=2,\n",
    "        random_state=42\n",
    "    )\n",
    "    fitted_model = gs.fit(X_train_data, y_train_data)\n",
    "    pred = fitted_model.predict(X_test_data)\n",
    "    return fitted_model, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE score for model:  8.329893986384073\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#FINDING NUM ESTIMATORS FOR HIGH LEARNING\n",
    "\n",
    "predictors = [x for x in train.columns if x not in [target]]\n",
    "xgb1 = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        booster='gbtree',\n",
    "        njobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        seed=27)\n",
    "evaluate_model(xgb1,predictors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.64844737693598\n",
      "MAE regular:  8.058605273564657\n",
      "Best params:  {'min_child_weight': 1, 'max_depth': 3}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>141.708105</td>\n",
       "      <td>0.692881</td>\n",
       "      <td>1.013658</td>\n",
       "      <td>0.087021</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>{'min_child_weight': 3, 'max_depth': 9}</td>\n",
       "      <td>-28.345465</td>\n",
       "      <td>-30.352578</td>\n",
       "      <td>-18.657508</td>\n",
       "      <td>-21.866098</td>\n",
       "      <td>-24.615925</td>\n",
       "      <td>-24.767596</td>\n",
       "      <td>4.237256</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>141.703105</td>\n",
       "      <td>0.553249</td>\n",
       "      <td>1.114464</td>\n",
       "      <td>0.165479</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>{'min_child_weight': 1, 'max_depth': 9}</td>\n",
       "      <td>-28.351963</td>\n",
       "      <td>-30.418186</td>\n",
       "      <td>-18.456531</td>\n",
       "      <td>-21.895110</td>\n",
       "      <td>-23.718032</td>\n",
       "      <td>-24.568059</td>\n",
       "      <td>4.330447</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>54.708329</td>\n",
       "      <td>0.111149</td>\n",
       "      <td>0.174810</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 1, 'max_depth': 3}</td>\n",
       "      <td>-29.018818</td>\n",
       "      <td>-30.799192</td>\n",
       "      <td>-17.509702</td>\n",
       "      <td>-21.176697</td>\n",
       "      <td>-19.736974</td>\n",
       "      <td>-23.648447</td>\n",
       "      <td>5.273935</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>109.747477</td>\n",
       "      <td>0.652376</td>\n",
       "      <td>0.570033</td>\n",
       "      <td>0.011064</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>{'min_child_weight': 5, 'max_depth': 7}</td>\n",
       "      <td>-28.791350</td>\n",
       "      <td>-31.734360</td>\n",
       "      <td>-18.689872</td>\n",
       "      <td>-21.792717</td>\n",
       "      <td>-25.407876</td>\n",
       "      <td>-25.283325</td>\n",
       "      <td>4.681876</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>81.829280</td>\n",
       "      <td>1.294951</td>\n",
       "      <td>0.347620</td>\n",
       "      <td>0.015501</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_child_weight': 5, 'max_depth': 5}</td>\n",
       "      <td>-28.604656</td>\n",
       "      <td>-32.303124</td>\n",
       "      <td>-18.375208</td>\n",
       "      <td>-21.994110</td>\n",
       "      <td>-23.540357</td>\n",
       "      <td>-24.963608</td>\n",
       "      <td>4.927096</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>58.629153</td>\n",
       "      <td>3.599356</td>\n",
       "      <td>0.181010</td>\n",
       "      <td>0.003347</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 5, 'max_depth': 3}</td>\n",
       "      <td>-28.919783</td>\n",
       "      <td>-31.098406</td>\n",
       "      <td>-17.696808</td>\n",
       "      <td>-21.349864</td>\n",
       "      <td>-19.683690</td>\n",
       "      <td>-23.749883</td>\n",
       "      <td>5.285204</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>56.820850</td>\n",
       "      <td>2.617714</td>\n",
       "      <td>0.177410</td>\n",
       "      <td>0.002728</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 3, 'max_depth': 3}</td>\n",
       "      <td>-29.031103</td>\n",
       "      <td>-32.080136</td>\n",
       "      <td>-17.740656</td>\n",
       "      <td>-21.200139</td>\n",
       "      <td>-19.600873</td>\n",
       "      <td>-23.930770</td>\n",
       "      <td>5.602719</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>140.304225</td>\n",
       "      <td>1.336001</td>\n",
       "      <td>0.907452</td>\n",
       "      <td>0.023020</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>{'min_child_weight': 5, 'max_depth': 9}</td>\n",
       "      <td>-28.052894</td>\n",
       "      <td>-31.672127</td>\n",
       "      <td>-18.588760</td>\n",
       "      <td>-21.855080</td>\n",
       "      <td>-24.182815</td>\n",
       "      <td>-24.870434</td>\n",
       "      <td>4.591733</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>86.212931</td>\n",
       "      <td>6.703987</td>\n",
       "      <td>0.355020</td>\n",
       "      <td>0.010734</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>{'min_child_weight': 3, 'max_depth': 5}</td>\n",
       "      <td>-28.843684</td>\n",
       "      <td>-32.925184</td>\n",
       "      <td>-18.441557</td>\n",
       "      <td>-21.697403</td>\n",
       "      <td>-22.468333</td>\n",
       "      <td>-24.875381</td>\n",
       "      <td>5.250515</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>113.846712</td>\n",
       "      <td>1.376301</td>\n",
       "      <td>0.634636</td>\n",
       "      <td>0.090171</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>{'min_child_weight': 3, 'max_depth': 7}</td>\n",
       "      <td>-28.469611</td>\n",
       "      <td>-30.738968</td>\n",
       "      <td>-18.744901</td>\n",
       "      <td>-22.083915</td>\n",
       "      <td>-25.018201</td>\n",
       "      <td>-25.011197</td>\n",
       "      <td>4.303656</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     141.708105      0.692881         1.013658        0.087021   \n",
       "1     141.703105      0.553249         1.114464        0.165479   \n",
       "2      54.708329      0.111149         0.174810        0.002482   \n",
       "3     109.747477      0.652376         0.570033        0.011064   \n",
       "4      81.829280      1.294951         0.347620        0.015501   \n",
       "5      58.629153      3.599356         0.181010        0.003347   \n",
       "6      56.820850      2.617714         0.177410        0.002728   \n",
       "7     140.304225      1.336001         0.907452        0.023020   \n",
       "8      86.212931      6.703987         0.355020        0.010734   \n",
       "9     113.846712      1.376301         0.634636        0.090171   \n",
       "\n",
       "  param_min_child_weight param_max_depth  \\\n",
       "0                      3               9   \n",
       "1                      1               9   \n",
       "2                      1               3   \n",
       "3                      5               7   \n",
       "4                      5               5   \n",
       "5                      5               3   \n",
       "6                      3               3   \n",
       "7                      5               9   \n",
       "8                      3               5   \n",
       "9                      3               7   \n",
       "\n",
       "                                    params  split0_test_score  \\\n",
       "0  {'min_child_weight': 3, 'max_depth': 9}         -28.345465   \n",
       "1  {'min_child_weight': 1, 'max_depth': 9}         -28.351963   \n",
       "2  {'min_child_weight': 1, 'max_depth': 3}         -29.018818   \n",
       "3  {'min_child_weight': 5, 'max_depth': 7}         -28.791350   \n",
       "4  {'min_child_weight': 5, 'max_depth': 5}         -28.604656   \n",
       "5  {'min_child_weight': 5, 'max_depth': 3}         -28.919783   \n",
       "6  {'min_child_weight': 3, 'max_depth': 3}         -29.031103   \n",
       "7  {'min_child_weight': 5, 'max_depth': 9}         -28.052894   \n",
       "8  {'min_child_weight': 3, 'max_depth': 5}         -28.843684   \n",
       "9  {'min_child_weight': 3, 'max_depth': 7}         -28.469611   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -30.352578         -18.657508         -21.866098         -24.615925   \n",
       "1         -30.418186         -18.456531         -21.895110         -23.718032   \n",
       "2         -30.799192         -17.509702         -21.176697         -19.736974   \n",
       "3         -31.734360         -18.689872         -21.792717         -25.407876   \n",
       "4         -32.303124         -18.375208         -21.994110         -23.540357   \n",
       "5         -31.098406         -17.696808         -21.349864         -19.683690   \n",
       "6         -32.080136         -17.740656         -21.200139         -19.600873   \n",
       "7         -31.672127         -18.588760         -21.855080         -24.182815   \n",
       "8         -32.925184         -18.441557         -21.697403         -22.468333   \n",
       "9         -30.738968         -18.744901         -22.083915         -25.018201   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -24.767596        4.237256                5  \n",
       "1       -24.568059        4.330447                4  \n",
       "2       -23.648447        5.273935                1  \n",
       "3       -25.283325        4.681876               10  \n",
       "4       -24.963608        4.927096                8  \n",
       "5       -23.749883        5.285204                2  \n",
       "6       -23.930770        5.602719                3  \n",
       "7       -24.870434        4.591733                6  \n",
       "8       -24.875381        5.250515                7  \n",
       "9       -25.011197        4.303656                9  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n_estimators_1 = 1000\n",
    "param_grid = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_grid)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 9 is smaller than n_iter=10. Running 9 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed: 11.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.15783890685888\n",
      "MAE regular:  5.6251672705014535\n",
      "Best params:  {'min_child_weight': 0.5, 'max_depth': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.821821</td>\n",
       "      <td>3.453418</td>\n",
       "      <td>0.119807</td>\n",
       "      <td>0.001600</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 0.5, 'max_depth': 2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>47.129296</td>\n",
       "      <td>2.073817</td>\n",
       "      <td>0.133008</td>\n",
       "      <td>0.008556</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 1, 'max_depth': 2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>49.385425</td>\n",
       "      <td>1.871452</td>\n",
       "      <td>0.131608</td>\n",
       "      <td>0.006249</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 2, 'max_depth': 2}</td>\n",
       "      <td>-28.679337</td>\n",
       "      <td>-29.986746</td>\n",
       "      <td>-17.270486</td>\n",
       "      <td>-21.143742</td>\n",
       "      <td>-19.090387</td>\n",
       "      <td>-23.234306</td>\n",
       "      <td>5.145049</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>61.040891</td>\n",
       "      <td>2.786082</td>\n",
       "      <td>0.184811</td>\n",
       "      <td>0.008281</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 0.5, 'max_depth': 3}</td>\n",
       "      <td>-29.018818</td>\n",
       "      <td>-30.799192</td>\n",
       "      <td>-17.509702</td>\n",
       "      <td>-21.176697</td>\n",
       "      <td>-19.736974</td>\n",
       "      <td>-23.648447</td>\n",
       "      <td>5.273935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>59.776619</td>\n",
       "      <td>2.192205</td>\n",
       "      <td>0.194611</td>\n",
       "      <td>0.006407</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 1, 'max_depth': 3}</td>\n",
       "      <td>-29.018818</td>\n",
       "      <td>-30.799192</td>\n",
       "      <td>-17.509702</td>\n",
       "      <td>-21.176697</td>\n",
       "      <td>-19.736974</td>\n",
       "      <td>-23.648447</td>\n",
       "      <td>5.273935</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>61.430114</td>\n",
       "      <td>3.126433</td>\n",
       "      <td>0.191811</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>{'min_child_weight': 2, 'max_depth': 3}</td>\n",
       "      <td>-29.216801</td>\n",
       "      <td>-31.200699</td>\n",
       "      <td>-17.685972</td>\n",
       "      <td>-21.251184</td>\n",
       "      <td>-19.892680</td>\n",
       "      <td>-23.849642</td>\n",
       "      <td>5.352559</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>73.664213</td>\n",
       "      <td>2.912449</td>\n",
       "      <td>0.266615</td>\n",
       "      <td>0.016428</td>\n",
       "      <td>0.5</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_child_weight': 0.5, 'max_depth': 4}</td>\n",
       "      <td>-28.649518</td>\n",
       "      <td>-32.648578</td>\n",
       "      <td>-17.958979</td>\n",
       "      <td>-21.671463</td>\n",
       "      <td>-22.067972</td>\n",
       "      <td>-24.599448</td>\n",
       "      <td>5.296685</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>76.965402</td>\n",
       "      <td>3.672574</td>\n",
       "      <td>0.308218</td>\n",
       "      <td>0.042190</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_child_weight': 1, 'max_depth': 4}</td>\n",
       "      <td>-28.649518</td>\n",
       "      <td>-32.648578</td>\n",
       "      <td>-17.958979</td>\n",
       "      <td>-21.671463</td>\n",
       "      <td>-22.067972</td>\n",
       "      <td>-24.599448</td>\n",
       "      <td>5.296685</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>73.775420</td>\n",
       "      <td>1.423909</td>\n",
       "      <td>0.266215</td>\n",
       "      <td>0.013076</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>{'min_child_weight': 2, 'max_depth': 4}</td>\n",
       "      <td>-28.929153</td>\n",
       "      <td>-32.425975</td>\n",
       "      <td>-18.128106</td>\n",
       "      <td>-21.617953</td>\n",
       "      <td>-22.789136</td>\n",
       "      <td>-24.778202</td>\n",
       "      <td>5.174775</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      45.821821      3.453418         0.119807        0.001600   \n",
       "1      47.129296      2.073817         0.133008        0.008556   \n",
       "2      49.385425      1.871452         0.131608        0.006249   \n",
       "3      61.040891      2.786082         0.184811        0.008281   \n",
       "4      59.776619      2.192205         0.194611        0.006407   \n",
       "5      61.430114      3.126433         0.191811        0.008796   \n",
       "6      73.664213      2.912449         0.266615        0.016428   \n",
       "7      76.965402      3.672574         0.308218        0.042190   \n",
       "8      73.775420      1.423909         0.266215        0.013076   \n",
       "\n",
       "  param_min_child_weight param_max_depth  \\\n",
       "0                    0.5               2   \n",
       "1                      1               2   \n",
       "2                      2               2   \n",
       "3                    0.5               3   \n",
       "4                      1               3   \n",
       "5                      2               3   \n",
       "6                    0.5               4   \n",
       "7                      1               4   \n",
       "8                      2               4   \n",
       "\n",
       "                                      params  split0_test_score  \\\n",
       "0  {'min_child_weight': 0.5, 'max_depth': 2}         -28.668579   \n",
       "1    {'min_child_weight': 1, 'max_depth': 2}         -28.668579   \n",
       "2    {'min_child_weight': 2, 'max_depth': 2}         -28.679337   \n",
       "3  {'min_child_weight': 0.5, 'max_depth': 3}         -29.018818   \n",
       "4    {'min_child_weight': 1, 'max_depth': 3}         -29.018818   \n",
       "5    {'min_child_weight': 2, 'max_depth': 3}         -29.216801   \n",
       "6  {'min_child_weight': 0.5, 'max_depth': 4}         -28.649518   \n",
       "7    {'min_child_weight': 1, 'max_depth': 4}         -28.649518   \n",
       "8    {'min_child_weight': 2, 'max_depth': 4}         -28.929153   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -29.754075         -17.139732         -21.109542         -19.116451   \n",
       "1         -29.754075         -17.139732         -21.109542         -19.116451   \n",
       "2         -29.986746         -17.270486         -21.143742         -19.090387   \n",
       "3         -30.799192         -17.509702         -21.176697         -19.736974   \n",
       "4         -30.799192         -17.509702         -21.176697         -19.736974   \n",
       "5         -31.200699         -17.685972         -21.251184         -19.892680   \n",
       "6         -32.648578         -17.958979         -21.671463         -22.067972   \n",
       "7         -32.648578         -17.958979         -21.671463         -22.067972   \n",
       "8         -32.425975         -18.128106         -21.617953         -22.789136   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -23.157839        5.111336                1  \n",
       "1       -23.157839        5.111336                1  \n",
       "2       -23.234306        5.145049                3  \n",
       "3       -23.648447        5.273935                4  \n",
       "4       -23.648447        5.273935                4  \n",
       "5       -23.849642        5.352559                6  \n",
       "6       -24.599448        5.296685                7  \n",
       "7       -24.599448        5.296685                7  \n",
       "8       -24.778202        5.174775                9  "
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test2 = {\n",
    " 'max_depth':[2,3,4],\n",
    " 'min_child_weight':[0.5,1,2]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test2)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed: 17.3min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed: 28.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.15783890685888\n",
      "MAE regular:  5.6251672705014535\n",
      "Best params:  {'min_child_weight': 0.5, 'max_depth': 2}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_min_child_weight</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>208.609532</td>\n",
       "      <td>8.577321</td>\n",
       "      <td>2.278130</td>\n",
       "      <td>0.381799</td>\n",
       "      <td>0.25</td>\n",
       "      <td>12</td>\n",
       "      <td>{'min_child_weight': 0.25, 'max_depth': 12}</td>\n",
       "      <td>-28.442560</td>\n",
       "      <td>-29.477263</td>\n",
       "      <td>-18.424859</td>\n",
       "      <td>-21.712392</td>\n",
       "      <td>-22.902268</td>\n",
       "      <td>-24.191969</td>\n",
       "      <td>4.173168</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>202.786999</td>\n",
       "      <td>8.353920</td>\n",
       "      <td>1.993314</td>\n",
       "      <td>0.282565</td>\n",
       "      <td>0.5</td>\n",
       "      <td>12</td>\n",
       "      <td>{'min_child_weight': 0.5, 'max_depth': 12}</td>\n",
       "      <td>-28.442560</td>\n",
       "      <td>-29.477263</td>\n",
       "      <td>-18.424859</td>\n",
       "      <td>-21.712392</td>\n",
       "      <td>-22.902268</td>\n",
       "      <td>-24.191969</td>\n",
       "      <td>4.173168</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>43.043062</td>\n",
       "      <td>0.204899</td>\n",
       "      <td>0.129607</td>\n",
       "      <td>0.012468</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 0.5, 'max_depth': 2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>160.088556</td>\n",
       "      <td>0.804583</td>\n",
       "      <td>1.091262</td>\n",
       "      <td>0.005845</td>\n",
       "      <td>0.75</td>\n",
       "      <td>10</td>\n",
       "      <td>{'min_child_weight': 0.75, 'max_depth': 10}</td>\n",
       "      <td>-28.394305</td>\n",
       "      <td>-30.296622</td>\n",
       "      <td>-18.445941</td>\n",
       "      <td>-22.103061</td>\n",
       "      <td>-24.453246</td>\n",
       "      <td>-24.738713</td>\n",
       "      <td>4.263444</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>127.588698</td>\n",
       "      <td>0.837705</td>\n",
       "      <td>0.825647</td>\n",
       "      <td>0.015820</td>\n",
       "      <td>0.75</td>\n",
       "      <td>8</td>\n",
       "      <td>{'min_child_weight': 0.75, 'max_depth': 8}</td>\n",
       "      <td>-28.751158</td>\n",
       "      <td>-31.175433</td>\n",
       "      <td>-18.669048</td>\n",
       "      <td>-21.837928</td>\n",
       "      <td>-24.313457</td>\n",
       "      <td>-24.949505</td>\n",
       "      <td>4.533200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>43.370881</td>\n",
       "      <td>0.310705</td>\n",
       "      <td>0.120007</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.75</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 0.75, 'max_depth': 2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>43.227272</td>\n",
       "      <td>0.192300</td>\n",
       "      <td>0.121007</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.25</td>\n",
       "      <td>2</td>\n",
       "      <td>{'min_child_weight': 0.25, 'max_depth': 2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>196.223823</td>\n",
       "      <td>2.346527</td>\n",
       "      <td>1.913510</td>\n",
       "      <td>0.079010</td>\n",
       "      <td>0.75</td>\n",
       "      <td>12</td>\n",
       "      <td>{'min_child_weight': 0.75, 'max_depth': 12}</td>\n",
       "      <td>-28.442560</td>\n",
       "      <td>-29.477263</td>\n",
       "      <td>-18.424859</td>\n",
       "      <td>-21.712392</td>\n",
       "      <td>-22.902268</td>\n",
       "      <td>-24.191969</td>\n",
       "      <td>4.173168</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>137.545067</td>\n",
       "      <td>7.120811</td>\n",
       "      <td>0.873050</td>\n",
       "      <td>0.108393</td>\n",
       "      <td>0.25</td>\n",
       "      <td>8</td>\n",
       "      <td>{'min_child_weight': 0.25, 'max_depth': 8}</td>\n",
       "      <td>-28.751158</td>\n",
       "      <td>-31.175433</td>\n",
       "      <td>-18.669048</td>\n",
       "      <td>-21.837928</td>\n",
       "      <td>-24.313457</td>\n",
       "      <td>-24.949505</td>\n",
       "      <td>4.533200</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>162.671504</td>\n",
       "      <td>3.356905</td>\n",
       "      <td>1.099863</td>\n",
       "      <td>0.058939</td>\n",
       "      <td>0.25</td>\n",
       "      <td>10</td>\n",
       "      <td>{'min_child_weight': 0.25, 'max_depth': 10}</td>\n",
       "      <td>-28.394305</td>\n",
       "      <td>-30.296622</td>\n",
       "      <td>-18.445941</td>\n",
       "      <td>-22.103061</td>\n",
       "      <td>-24.453246</td>\n",
       "      <td>-24.738713</td>\n",
       "      <td>4.263444</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     208.609532      8.577321         2.278130        0.381799   \n",
       "1     202.786999      8.353920         1.993314        0.282565   \n",
       "2      43.043062      0.204899         0.129607        0.012468   \n",
       "3     160.088556      0.804583         1.091262        0.005845   \n",
       "4     127.588698      0.837705         0.825647        0.015820   \n",
       "5      43.370881      0.310705         0.120007        0.001549   \n",
       "6      43.227272      0.192300         0.121007        0.001789   \n",
       "7     196.223823      2.346527         1.913510        0.079010   \n",
       "8     137.545067      7.120811         0.873050        0.108393   \n",
       "9     162.671504      3.356905         1.099863        0.058939   \n",
       "\n",
       "  param_min_child_weight param_max_depth  \\\n",
       "0                   0.25              12   \n",
       "1                    0.5              12   \n",
       "2                    0.5               2   \n",
       "3                   0.75              10   \n",
       "4                   0.75               8   \n",
       "5                   0.75               2   \n",
       "6                   0.25               2   \n",
       "7                   0.75              12   \n",
       "8                   0.25               8   \n",
       "9                   0.25              10   \n",
       "\n",
       "                                        params  split0_test_score  \\\n",
       "0  {'min_child_weight': 0.25, 'max_depth': 12}         -28.442560   \n",
       "1   {'min_child_weight': 0.5, 'max_depth': 12}         -28.442560   \n",
       "2    {'min_child_weight': 0.5, 'max_depth': 2}         -28.668579   \n",
       "3  {'min_child_weight': 0.75, 'max_depth': 10}         -28.394305   \n",
       "4   {'min_child_weight': 0.75, 'max_depth': 8}         -28.751158   \n",
       "5   {'min_child_weight': 0.75, 'max_depth': 2}         -28.668579   \n",
       "6   {'min_child_weight': 0.25, 'max_depth': 2}         -28.668579   \n",
       "7  {'min_child_weight': 0.75, 'max_depth': 12}         -28.442560   \n",
       "8   {'min_child_weight': 0.25, 'max_depth': 8}         -28.751158   \n",
       "9  {'min_child_weight': 0.25, 'max_depth': 10}         -28.394305   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -29.477263         -18.424859         -21.712392         -22.902268   \n",
       "1         -29.477263         -18.424859         -21.712392         -22.902268   \n",
       "2         -29.754075         -17.139732         -21.109542         -19.116451   \n",
       "3         -30.296622         -18.445941         -22.103061         -24.453246   \n",
       "4         -31.175433         -18.669048         -21.837928         -24.313457   \n",
       "5         -29.754075         -17.139732         -21.109542         -19.116451   \n",
       "6         -29.754075         -17.139732         -21.109542         -19.116451   \n",
       "7         -29.477263         -18.424859         -21.712392         -22.902268   \n",
       "8         -31.175433         -18.669048         -21.837928         -24.313457   \n",
       "9         -30.296622         -18.445941         -22.103061         -24.453246   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -24.191969        4.173168                4  \n",
       "1       -24.191969        4.173168                4  \n",
       "2       -23.157839        5.111336                1  \n",
       "3       -24.738713        4.263444                7  \n",
       "4       -24.949505        4.533200                9  \n",
       "5       -23.157839        5.111336                1  \n",
       "6       -23.157839        5.111336                1  \n",
       "7       -24.191969        4.173168                4  \n",
       "8       -24.949505        4.533200                9  \n",
       "9       -24.738713        4.263444                7  "
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_depth_best_1=3\n",
    "min_child_weight_best=0.5\n",
    "param_test3 = {\n",
    " 'min_child_weight':[0.5,0.25,0.75],\n",
    " 'max_depth':[2,8,10,12]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=max_depth_best_1,\n",
    "        min_child_weight=1,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test3)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  25 out of  25 | elapsed:  5.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.15783890685888\n",
      "MAE regular:  5.6251672705014535\n",
      "Best params:  {'gamma': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>42.963457</td>\n",
       "      <td>0.096001</td>\n",
       "      <td>0.120207</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0</td>\n",
       "      <td>{'gamma': 0.0}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>42.819649</td>\n",
       "      <td>0.196587</td>\n",
       "      <td>0.120007</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'gamma': 0.1}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>43.127267</td>\n",
       "      <td>0.195843</td>\n",
       "      <td>0.120607</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'gamma': 0.2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>42.875252</td>\n",
       "      <td>0.375257</td>\n",
       "      <td>0.119407</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'gamma': 0.3}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>42.434227</td>\n",
       "      <td>0.505571</td>\n",
       "      <td>0.122407</td>\n",
       "      <td>0.008755</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'gamma': 0.4}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_gamma  \\\n",
       "0      42.963457      0.096001         0.120207        0.001939           0   \n",
       "1      42.819649      0.196587         0.120007        0.002450         0.1   \n",
       "2      43.127267      0.195843         0.120607        0.001357         0.2   \n",
       "3      42.875252      0.375257         0.119407        0.001020         0.3   \n",
       "4      42.434227      0.505571         0.122407        0.008755         0.4   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'gamma': 0.0}         -28.668579         -29.754075         -17.139732   \n",
       "1  {'gamma': 0.1}         -28.668579         -29.754075         -17.139732   \n",
       "2  {'gamma': 0.2}         -28.668579         -29.754075         -17.139732   \n",
       "3  {'gamma': 0.3}         -28.668579         -29.754075         -17.139732   \n",
       "4  {'gamma': 0.4}         -28.668579         -29.754075         -17.139732   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "1         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "2         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "3         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "4         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  "
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test4 = {\n",
    " 'gamma':[i/10.0 for i in range(0,5)]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test4)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  9.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.15783890685888\n",
      "MAE regular:  5.6251672705014535\n",
      "Best params:  {'gamma': 0.0}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_gamma</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>42.852651</td>\n",
       "      <td>0.144313</td>\n",
       "      <td>0.120007</td>\n",
       "      <td>0.003406</td>\n",
       "      <td>0</td>\n",
       "      <td>{'gamma': 0.0}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>42.796248</td>\n",
       "      <td>0.144166</td>\n",
       "      <td>0.123607</td>\n",
       "      <td>0.008823</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'gamma': 0.1}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>42.680041</td>\n",
       "      <td>0.081022</td>\n",
       "      <td>0.119607</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.2</td>\n",
       "      <td>{'gamma': 0.2}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>42.685642</td>\n",
       "      <td>0.056329</td>\n",
       "      <td>0.120007</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>0.3</td>\n",
       "      <td>{'gamma': 0.3}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>42.701242</td>\n",
       "      <td>0.051568</td>\n",
       "      <td>0.118207</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.4</td>\n",
       "      <td>{'gamma': 0.4}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>42.724844</td>\n",
       "      <td>0.095419</td>\n",
       "      <td>0.118207</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'gamma': 0.5}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>42.809649</td>\n",
       "      <td>0.154436</td>\n",
       "      <td>0.127407</td>\n",
       "      <td>0.017317</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'gamma': 0.6}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>42.758446</td>\n",
       "      <td>0.188208</td>\n",
       "      <td>0.118807</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'gamma': 0.7}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>42.686042</td>\n",
       "      <td>0.070228</td>\n",
       "      <td>0.118607</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'gamma': 0.8}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>42.404225</td>\n",
       "      <td>0.501995</td>\n",
       "      <td>0.117407</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'gamma': 0.9}</td>\n",
       "      <td>-28.668579</td>\n",
       "      <td>-29.754075</td>\n",
       "      <td>-17.139732</td>\n",
       "      <td>-21.109542</td>\n",
       "      <td>-19.116451</td>\n",
       "      <td>-23.157839</td>\n",
       "      <td>5.111336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time param_gamma  \\\n",
       "0      42.852651      0.144313         0.120007        0.003406           0   \n",
       "1      42.796248      0.144166         0.123607        0.008823         0.1   \n",
       "2      42.680041      0.081022         0.119607        0.000490         0.2   \n",
       "3      42.685642      0.056329         0.120007        0.004858         0.3   \n",
       "4      42.701242      0.051568         0.118207        0.002040         0.4   \n",
       "5      42.724844      0.095419         0.118207        0.001470         0.5   \n",
       "6      42.809649      0.154436         0.127407        0.017317         0.6   \n",
       "7      42.758446      0.188208         0.118807        0.001721         0.7   \n",
       "8      42.686042      0.070228         0.118607        0.002577         0.8   \n",
       "9      42.404225      0.501995         0.117407        0.001497         0.9   \n",
       "\n",
       "           params  split0_test_score  split1_test_score  split2_test_score  \\\n",
       "0  {'gamma': 0.0}         -28.668579         -29.754075         -17.139732   \n",
       "1  {'gamma': 0.1}         -28.668579         -29.754075         -17.139732   \n",
       "2  {'gamma': 0.2}         -28.668579         -29.754075         -17.139732   \n",
       "3  {'gamma': 0.3}         -28.668579         -29.754075         -17.139732   \n",
       "4  {'gamma': 0.4}         -28.668579         -29.754075         -17.139732   \n",
       "5  {'gamma': 0.5}         -28.668579         -29.754075         -17.139732   \n",
       "6  {'gamma': 0.6}         -28.668579         -29.754075         -17.139732   \n",
       "7  {'gamma': 0.7}         -28.668579         -29.754075         -17.139732   \n",
       "8  {'gamma': 0.8}         -28.668579         -29.754075         -17.139732   \n",
       "9  {'gamma': 0.9}         -28.668579         -29.754075         -17.139732   \n",
       "\n",
       "   split3_test_score  split4_test_score  mean_test_score  std_test_score  \\\n",
       "0         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "1         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "2         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "3         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "4         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "5         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "6         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "7         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "8         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "9         -21.109542         -19.116451       -23.157839        5.111336   \n",
       "\n",
       "   rank_test_score  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "5                1  \n",
       "6                1  \n",
       "7                1  \n",
       "8                1  \n",
       "9                1  "
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test5 = {\n",
    " 'gamma':[i/100.0 for i in range(0,100,10)]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test5)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  9.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.07657677657708\n",
      "MAE regular:  6.046934564908344\n",
      "Best params:  {'subsample': 0.9, 'colsample_bytree': 0.8}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>37.656954</td>\n",
       "      <td>0.108806</td>\n",
       "      <td>0.117807</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'colsample_bytree': 0.6}</td>\n",
       "      <td>-28.669301</td>\n",
       "      <td>-30.281318</td>\n",
       "      <td>-17.248411</td>\n",
       "      <td>-21.272403</td>\n",
       "      <td>-18.975549</td>\n",
       "      <td>-23.289565</td>\n",
       "      <td>5.234609</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>37.725958</td>\n",
       "      <td>0.386163</td>\n",
       "      <td>0.118607</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.6}</td>\n",
       "      <td>-28.552206</td>\n",
       "      <td>-29.840166</td>\n",
       "      <td>-17.259416</td>\n",
       "      <td>-21.192833</td>\n",
       "      <td>-19.173205</td>\n",
       "      <td>-23.203727</td>\n",
       "      <td>5.065103</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>40.110694</td>\n",
       "      <td>0.085125</td>\n",
       "      <td>0.117807</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.7}</td>\n",
       "      <td>-28.845961</td>\n",
       "      <td>-30.010706</td>\n",
       "      <td>-17.296950</td>\n",
       "      <td>-21.152977</td>\n",
       "      <td>-19.499073</td>\n",
       "      <td>-23.361296</td>\n",
       "      <td>5.116059</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>45.291790</td>\n",
       "      <td>0.125566</td>\n",
       "      <td>0.117807</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'subsample': 0.8, 'colsample_bytree': 0.9}</td>\n",
       "      <td>-28.568706</td>\n",
       "      <td>-29.688745</td>\n",
       "      <td>-17.189350</td>\n",
       "      <td>-21.157220</td>\n",
       "      <td>-19.364103</td>\n",
       "      <td>-23.193782</td>\n",
       "      <td>5.018880</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45.344394</td>\n",
       "      <td>0.019733</td>\n",
       "      <td>0.137408</td>\n",
       "      <td>0.039370</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.9}</td>\n",
       "      <td>-28.816297</td>\n",
       "      <td>-29.552213</td>\n",
       "      <td>-17.418621</td>\n",
       "      <td>-21.190544</td>\n",
       "      <td>-19.475990</td>\n",
       "      <td>-23.290891</td>\n",
       "      <td>4.963604</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>42.647439</td>\n",
       "      <td>0.114947</td>\n",
       "      <td>0.119807</td>\n",
       "      <td>0.001939</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.9, 'colsample_bytree': 0.8}</td>\n",
       "      <td>-28.712535</td>\n",
       "      <td>-29.108291</td>\n",
       "      <td>-17.067292</td>\n",
       "      <td>-21.183151</td>\n",
       "      <td>-19.310858</td>\n",
       "      <td>-23.076577</td>\n",
       "      <td>4.940181</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>42.786247</td>\n",
       "      <td>0.108142</td>\n",
       "      <td>0.115607</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.6, 'colsample_bytree': 0.8}</td>\n",
       "      <td>-28.703647</td>\n",
       "      <td>-29.995216</td>\n",
       "      <td>-17.229467</td>\n",
       "      <td>-21.266639</td>\n",
       "      <td>-19.061067</td>\n",
       "      <td>-23.251372</td>\n",
       "      <td>5.156979</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>42.859251</td>\n",
       "      <td>0.207844</td>\n",
       "      <td>0.118007</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.8}</td>\n",
       "      <td>-28.709160</td>\n",
       "      <td>-29.917556</td>\n",
       "      <td>-17.078899</td>\n",
       "      <td>-21.179485</td>\n",
       "      <td>-19.236738</td>\n",
       "      <td>-23.224529</td>\n",
       "      <td>5.152391</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>37.521146</td>\n",
       "      <td>0.103238</td>\n",
       "      <td>0.119207</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.8, 'colsample_bytree': 0.6}</td>\n",
       "      <td>-28.645676</td>\n",
       "      <td>-29.706050</td>\n",
       "      <td>-17.145495</td>\n",
       "      <td>-21.177260</td>\n",
       "      <td>-18.999482</td>\n",
       "      <td>-23.134956</td>\n",
       "      <td>5.106072</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>44.916369</td>\n",
       "      <td>0.389407</td>\n",
       "      <td>0.120207</td>\n",
       "      <td>0.003544</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'subsample': 0.9, 'colsample_bytree': 0.9}</td>\n",
       "      <td>-28.552105</td>\n",
       "      <td>-29.729514</td>\n",
       "      <td>-17.096024</td>\n",
       "      <td>-21.411863</td>\n",
       "      <td>-19.071610</td>\n",
       "      <td>-23.172380</td>\n",
       "      <td>5.075027</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      37.656954      0.108806         0.117807        0.002638   \n",
       "1      37.725958      0.386163         0.118607        0.002653   \n",
       "2      40.110694      0.085125         0.117807        0.001327   \n",
       "3      45.291790      0.125566         0.117807        0.002315   \n",
       "4      45.344394      0.019733         0.137408        0.039370   \n",
       "5      42.647439      0.114947         0.119807        0.001939   \n",
       "6      42.786247      0.108142         0.115607        0.003200   \n",
       "7      42.859251      0.207844         0.118007        0.001414   \n",
       "8      37.521146      0.103238         0.119207        0.002040   \n",
       "9      44.916369      0.389407         0.120207        0.003544   \n",
       "\n",
       "  param_subsample param_colsample_bytree  \\\n",
       "0             0.6                    0.6   \n",
       "1             0.7                    0.6   \n",
       "2             0.7                    0.7   \n",
       "3             0.8                    0.9   \n",
       "4             0.7                    0.9   \n",
       "5             0.9                    0.8   \n",
       "6             0.6                    0.8   \n",
       "7             0.7                    0.8   \n",
       "8             0.8                    0.6   \n",
       "9             0.9                    0.9   \n",
       "\n",
       "                                        params  split0_test_score  \\\n",
       "0  {'subsample': 0.6, 'colsample_bytree': 0.6}         -28.669301   \n",
       "1  {'subsample': 0.7, 'colsample_bytree': 0.6}         -28.552206   \n",
       "2  {'subsample': 0.7, 'colsample_bytree': 0.7}         -28.845961   \n",
       "3  {'subsample': 0.8, 'colsample_bytree': 0.9}         -28.568706   \n",
       "4  {'subsample': 0.7, 'colsample_bytree': 0.9}         -28.816297   \n",
       "5  {'subsample': 0.9, 'colsample_bytree': 0.8}         -28.712535   \n",
       "6  {'subsample': 0.6, 'colsample_bytree': 0.8}         -28.703647   \n",
       "7  {'subsample': 0.7, 'colsample_bytree': 0.8}         -28.709160   \n",
       "8  {'subsample': 0.8, 'colsample_bytree': 0.6}         -28.645676   \n",
       "9  {'subsample': 0.9, 'colsample_bytree': 0.9}         -28.552105   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -30.281318         -17.248411         -21.272403         -18.975549   \n",
       "1         -29.840166         -17.259416         -21.192833         -19.173205   \n",
       "2         -30.010706         -17.296950         -21.152977         -19.499073   \n",
       "3         -29.688745         -17.189350         -21.157220         -19.364103   \n",
       "4         -29.552213         -17.418621         -21.190544         -19.475990   \n",
       "5         -29.108291         -17.067292         -21.183151         -19.310858   \n",
       "6         -29.995216         -17.229467         -21.266639         -19.061067   \n",
       "7         -29.917556         -17.078899         -21.179485         -19.236738   \n",
       "8         -29.706050         -17.145495         -21.177260         -18.999482   \n",
       "9         -29.729514         -17.096024         -21.411863         -19.071610   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -23.289565        5.234609                8  \n",
       "1       -23.203727        5.065103                5  \n",
       "2       -23.361296        5.116059               10  \n",
       "3       -23.193782        5.018880                4  \n",
       "4       -23.290891        4.963604                9  \n",
       "5       -23.076577        4.940181                1  \n",
       "6       -23.251372        5.156979                7  \n",
       "7       -23.224529        5.152391                6  \n",
       "8       -23.134956        5.106072                2  \n",
       "9       -23.172380        5.075027                3  "
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test6 = {\n",
    " 'subsample':[i/10.0 for i in range(6,10)],\n",
    " 'colsample_bytree':[i/10.0 for i in range(6,10)]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test6)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  5.9min\n",
      "[Parallel(n_jobs=-1)]: Done  50 out of  50 | elapsed:  8.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.172380055148007\n",
      "MAE regular:  5.623965462048847\n",
      "Best params:  {'subsample': 0.9, 'colsample_bytree': 0.9}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_subsample</th>\n",
       "      <th>param_colsample_bytree</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>40.711729</td>\n",
       "      <td>0.700078</td>\n",
       "      <td>0.120407</td>\n",
       "      <td>0.002654</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.7}</td>\n",
       "      <td>-28.845961</td>\n",
       "      <td>-30.010706</td>\n",
       "      <td>-17.296950</td>\n",
       "      <td>-21.152977</td>\n",
       "      <td>-19.499073</td>\n",
       "      <td>-23.361296</td>\n",
       "      <td>5.116059</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>36.461885</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>0.119007</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'subsample': 0.55, 'colsample_bytree': 0.55}</td>\n",
       "      <td>-28.547297</td>\n",
       "      <td>-30.380481</td>\n",
       "      <td>-17.417197</td>\n",
       "      <td>-21.125200</td>\n",
       "      <td>-19.131642</td>\n",
       "      <td>-23.320534</td>\n",
       "      <td>5.184242</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>39.147839</td>\n",
       "      <td>0.479202</td>\n",
       "      <td>0.119607</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'subsample': 0.75, 'colsample_bytree': 0.65}</td>\n",
       "      <td>-28.649755</td>\n",
       "      <td>-30.855148</td>\n",
       "      <td>-17.062368</td>\n",
       "      <td>-21.225347</td>\n",
       "      <td>-19.161674</td>\n",
       "      <td>-23.391029</td>\n",
       "      <td>5.403733</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>40.317306</td>\n",
       "      <td>0.099176</td>\n",
       "      <td>0.119807</td>\n",
       "      <td>0.002400</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'subsample': 0.75, 'colsample_bytree': 0.7}</td>\n",
       "      <td>-28.803347</td>\n",
       "      <td>-30.118582</td>\n",
       "      <td>-17.233617</td>\n",
       "      <td>-21.089706</td>\n",
       "      <td>-19.280210</td>\n",
       "      <td>-23.305259</td>\n",
       "      <td>5.189013</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>39.051834</td>\n",
       "      <td>0.112865</td>\n",
       "      <td>0.118807</td>\n",
       "      <td>0.003655</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.65</td>\n",
       "      <td>{'subsample': 0.55, 'colsample_bytree': 0.65}</td>\n",
       "      <td>-28.642717</td>\n",
       "      <td>-29.585499</td>\n",
       "      <td>-17.530026</td>\n",
       "      <td>-21.124210</td>\n",
       "      <td>-19.196695</td>\n",
       "      <td>-23.215993</td>\n",
       "      <td>4.957505</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>40.424112</td>\n",
       "      <td>0.148710</td>\n",
       "      <td>0.117407</td>\n",
       "      <td>0.002939</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>{'subsample': 0.6, 'colsample_bytree': 0.7}</td>\n",
       "      <td>-28.601473</td>\n",
       "      <td>-30.086402</td>\n",
       "      <td>-17.265075</td>\n",
       "      <td>-21.387107</td>\n",
       "      <td>-19.246460</td>\n",
       "      <td>-23.317464</td>\n",
       "      <td>5.112227</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>37.865166</td>\n",
       "      <td>0.099864</td>\n",
       "      <td>0.123607</td>\n",
       "      <td>0.015731</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.6, 'colsample_bytree': 0.6}</td>\n",
       "      <td>-28.669301</td>\n",
       "      <td>-30.281318</td>\n",
       "      <td>-17.248411</td>\n",
       "      <td>-21.272403</td>\n",
       "      <td>-18.975549</td>\n",
       "      <td>-23.289565</td>\n",
       "      <td>5.234609</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>45.443199</td>\n",
       "      <td>0.110095</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.003868</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>{'subsample': 0.9, 'colsample_bytree': 0.9}</td>\n",
       "      <td>-28.552105</td>\n",
       "      <td>-29.729514</td>\n",
       "      <td>-17.096024</td>\n",
       "      <td>-21.411863</td>\n",
       "      <td>-19.071610</td>\n",
       "      <td>-23.172380</td>\n",
       "      <td>5.075027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>36.493687</td>\n",
       "      <td>0.181884</td>\n",
       "      <td>0.121207</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.55</td>\n",
       "      <td>{'subsample': 0.75, 'colsample_bytree': 0.55}</td>\n",
       "      <td>-28.683537</td>\n",
       "      <td>-30.828578</td>\n",
       "      <td>-17.286220</td>\n",
       "      <td>-21.128240</td>\n",
       "      <td>-19.187412</td>\n",
       "      <td>-23.422972</td>\n",
       "      <td>5.355121</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>37.494145</td>\n",
       "      <td>0.377150</td>\n",
       "      <td>0.117407</td>\n",
       "      <td>0.002871</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>{'subsample': 0.7, 'colsample_bytree': 0.6}</td>\n",
       "      <td>-28.552206</td>\n",
       "      <td>-29.840166</td>\n",
       "      <td>-17.259416</td>\n",
       "      <td>-21.192833</td>\n",
       "      <td>-19.173205</td>\n",
       "      <td>-23.203727</td>\n",
       "      <td>5.065103</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      40.711729      0.700078         0.120407        0.002654   \n",
       "1      36.461885      0.059365         0.119007        0.002450   \n",
       "2      39.147839      0.479202         0.119607        0.002245   \n",
       "3      40.317306      0.099176         0.119807        0.002400   \n",
       "4      39.051834      0.112865         0.118807        0.003655   \n",
       "5      40.424112      0.148710         0.117407        0.002939   \n",
       "6      37.865166      0.099864         0.123607        0.015731   \n",
       "7      45.443199      0.110095         0.122807        0.003868   \n",
       "8      36.493687      0.181884         0.121207        0.003430   \n",
       "9      37.494145      0.377150         0.117407        0.002871   \n",
       "\n",
       "  param_subsample param_colsample_bytree  \\\n",
       "0             0.7                    0.7   \n",
       "1            0.55                   0.55   \n",
       "2            0.75                   0.65   \n",
       "3            0.75                    0.7   \n",
       "4            0.55                   0.65   \n",
       "5             0.6                    0.7   \n",
       "6             0.6                    0.6   \n",
       "7             0.9                    0.9   \n",
       "8            0.75                   0.55   \n",
       "9             0.7                    0.6   \n",
       "\n",
       "                                          params  split0_test_score  \\\n",
       "0    {'subsample': 0.7, 'colsample_bytree': 0.7}         -28.845961   \n",
       "1  {'subsample': 0.55, 'colsample_bytree': 0.55}         -28.547297   \n",
       "2  {'subsample': 0.75, 'colsample_bytree': 0.65}         -28.649755   \n",
       "3   {'subsample': 0.75, 'colsample_bytree': 0.7}         -28.803347   \n",
       "4  {'subsample': 0.55, 'colsample_bytree': 0.65}         -28.642717   \n",
       "5    {'subsample': 0.6, 'colsample_bytree': 0.7}         -28.601473   \n",
       "6    {'subsample': 0.6, 'colsample_bytree': 0.6}         -28.669301   \n",
       "7    {'subsample': 0.9, 'colsample_bytree': 0.9}         -28.552105   \n",
       "8  {'subsample': 0.75, 'colsample_bytree': 0.55}         -28.683537   \n",
       "9    {'subsample': 0.7, 'colsample_bytree': 0.6}         -28.552206   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -30.010706         -17.296950         -21.152977         -19.499073   \n",
       "1         -30.380481         -17.417197         -21.125200         -19.131642   \n",
       "2         -30.855148         -17.062368         -21.225347         -19.161674   \n",
       "3         -30.118582         -17.233617         -21.089706         -19.280210   \n",
       "4         -29.585499         -17.530026         -21.124210         -19.196695   \n",
       "5         -30.086402         -17.265075         -21.387107         -19.246460   \n",
       "6         -30.281318         -17.248411         -21.272403         -18.975549   \n",
       "7         -29.729514         -17.096024         -21.411863         -19.071610   \n",
       "8         -30.828578         -17.286220         -21.128240         -19.187412   \n",
       "9         -29.840166         -17.259416         -21.192833         -19.173205   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -23.361296        5.116059                8  \n",
       "1       -23.320534        5.184242                7  \n",
       "2       -23.391029        5.403733                9  \n",
       "3       -23.305259        5.189013                5  \n",
       "4       -23.215993        4.957505                3  \n",
       "5       -23.317464        5.112227                6  \n",
       "6       -23.289565        5.234609                4  \n",
       "7       -23.172380        5.075027                1  \n",
       "8       -23.422972        5.355121               10  \n",
       "9       -23.203727        5.065103                2  "
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test7 = {\n",
    " 'subsample':[i/100.0 for i in range(55,96,5)],\n",
    " 'colsample_bytree':[i/100.0 for i in range(55,96,5)]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.8,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test7)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  23.049843245357085\n",
      "MAE regular:  6.95161493619283\n",
      "Best params:  {'reg_alpha': 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_reg_alpha</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.555006</td>\n",
       "      <td>0.213063</td>\n",
       "      <td>0.125807</td>\n",
       "      <td>0.006178</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'reg_alpha': 1e-05}</td>\n",
       "      <td>-28.552106</td>\n",
       "      <td>-29.729515</td>\n",
       "      <td>-17.096024</td>\n",
       "      <td>-21.411862</td>\n",
       "      <td>-19.071610</td>\n",
       "      <td>-23.172380</td>\n",
       "      <td>5.075028</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>45.232787</td>\n",
       "      <td>0.130862</td>\n",
       "      <td>0.120807</td>\n",
       "      <td>0.003311</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'reg_alpha': 0.01}</td>\n",
       "      <td>-28.554756</td>\n",
       "      <td>-29.729489</td>\n",
       "      <td>-17.096022</td>\n",
       "      <td>-21.411857</td>\n",
       "      <td>-19.071609</td>\n",
       "      <td>-23.172903</td>\n",
       "      <td>5.075584</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>45.484202</td>\n",
       "      <td>0.232006</td>\n",
       "      <td>0.121607</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'reg_alpha': 0.1}</td>\n",
       "      <td>-28.554633</td>\n",
       "      <td>-30.324424</td>\n",
       "      <td>-17.162404</td>\n",
       "      <td>-21.354472</td>\n",
       "      <td>-19.071415</td>\n",
       "      <td>-23.293635</td>\n",
       "      <td>5.220996</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>45.590007</td>\n",
       "      <td>0.112351</td>\n",
       "      <td>0.123007</td>\n",
       "      <td>0.003162</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'reg_alpha': 0.5}</td>\n",
       "      <td>-28.604712</td>\n",
       "      <td>-29.856005</td>\n",
       "      <td>-17.118189</td>\n",
       "      <td>-21.439924</td>\n",
       "      <td>-19.132010</td>\n",
       "      <td>-23.230325</td>\n",
       "      <td>5.101919</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45.872224</td>\n",
       "      <td>0.241007</td>\n",
       "      <td>0.120807</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>1</td>\n",
       "      <td>{'reg_alpha': 1}</td>\n",
       "      <td>-28.575807</td>\n",
       "      <td>-29.810240</td>\n",
       "      <td>-17.215465</td>\n",
       "      <td>-21.347577</td>\n",
       "      <td>-19.172224</td>\n",
       "      <td>-23.224421</td>\n",
       "      <td>5.060927</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>45.630010</td>\n",
       "      <td>0.230732</td>\n",
       "      <td>0.121207</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>5</td>\n",
       "      <td>{'reg_alpha': 5}</td>\n",
       "      <td>-28.730978</td>\n",
       "      <td>-29.981231</td>\n",
       "      <td>-17.179835</td>\n",
       "      <td>-21.331846</td>\n",
       "      <td>-19.336533</td>\n",
       "      <td>-23.312244</td>\n",
       "      <td>5.122053</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>45.573407</td>\n",
       "      <td>0.262407</td>\n",
       "      <td>0.123207</td>\n",
       "      <td>0.002136</td>\n",
       "      <td>10</td>\n",
       "      <td>{'reg_alpha': 10}</td>\n",
       "      <td>-28.555240</td>\n",
       "      <td>-29.641095</td>\n",
       "      <td>-17.135391</td>\n",
       "      <td>-21.385676</td>\n",
       "      <td>-19.312079</td>\n",
       "      <td>-23.206049</td>\n",
       "      <td>5.007146</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>45.577807</td>\n",
       "      <td>0.145844</td>\n",
       "      <td>0.121807</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>100</td>\n",
       "      <td>{'reg_alpha': 100}</td>\n",
       "      <td>-28.701648</td>\n",
       "      <td>-28.802880</td>\n",
       "      <td>-17.343811</td>\n",
       "      <td>-21.230753</td>\n",
       "      <td>-19.169361</td>\n",
       "      <td>-23.049843</td>\n",
       "      <td>4.816017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      45.555006      0.213063         0.125807        0.006178   \n",
       "1      45.232787      0.130862         0.120807        0.003311   \n",
       "2      45.484202      0.232006         0.121607        0.002332   \n",
       "3      45.590007      0.112351         0.123007        0.003162   \n",
       "4      45.872224      0.241007         0.120807        0.002040   \n",
       "5      45.630010      0.230732         0.121207        0.001470   \n",
       "6      45.573407      0.262407         0.123207        0.002136   \n",
       "7      45.577807      0.145844         0.121807        0.003371   \n",
       "\n",
       "  param_reg_alpha                params  split0_test_score  split1_test_score  \\\n",
       "0           1e-05  {'reg_alpha': 1e-05}         -28.552106         -29.729515   \n",
       "1            0.01   {'reg_alpha': 0.01}         -28.554756         -29.729489   \n",
       "2             0.1    {'reg_alpha': 0.1}         -28.554633         -30.324424   \n",
       "3             0.5    {'reg_alpha': 0.5}         -28.604712         -29.856005   \n",
       "4               1      {'reg_alpha': 1}         -28.575807         -29.810240   \n",
       "5               5      {'reg_alpha': 5}         -28.730978         -29.981231   \n",
       "6              10     {'reg_alpha': 10}         -28.555240         -29.641095   \n",
       "7             100    {'reg_alpha': 100}         -28.701648         -28.802880   \n",
       "\n",
       "   split2_test_score  split3_test_score  split4_test_score  mean_test_score  \\\n",
       "0         -17.096024         -21.411862         -19.071610       -23.172380   \n",
       "1         -17.096022         -21.411857         -19.071609       -23.172903   \n",
       "2         -17.162404         -21.354472         -19.071415       -23.293635   \n",
       "3         -17.118189         -21.439924         -19.132010       -23.230325   \n",
       "4         -17.215465         -21.347577         -19.172224       -23.224421   \n",
       "5         -17.179835         -21.331846         -19.336533       -23.312244   \n",
       "6         -17.135391         -21.385676         -19.312079       -23.206049   \n",
       "7         -17.343811         -21.230753         -19.169361       -23.049843   \n",
       "\n",
       "   std_test_score  rank_test_score  \n",
       "0        5.075028                2  \n",
       "1        5.075584                3  \n",
       "2        5.220996                7  \n",
       "3        5.101919                6  \n",
       "4        5.060927                5  \n",
       "5        5.122053                8  \n",
       "6        5.007146                4  \n",
       "7        4.816017                1  "
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test8 = {\n",
    " 'reg_alpha':[1e-5, 1e-2, 0.1, 0.5,1,5,10, 100]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=best_n_estimators_1,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test8)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\Anaconda\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n",
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  6.8min\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  7.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE best score:  22.56236091697391\n",
      "MAE regular:  6.633733669916789\n",
      "Best params:  {'reg_lambda': 100}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_reg_lambda</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>45.700214</td>\n",
       "      <td>0.385310</td>\n",
       "      <td>0.120407</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>1e-05</td>\n",
       "      <td>{'reg_lambda': 1e-05}</td>\n",
       "      <td>-28.762461</td>\n",
       "      <td>-29.740717</td>\n",
       "      <td>-17.221824</td>\n",
       "      <td>-21.384143</td>\n",
       "      <td>-19.311311</td>\n",
       "      <td>-23.284248</td>\n",
       "      <td>5.056653</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>45.292391</td>\n",
       "      <td>0.243205</td>\n",
       "      <td>0.129207</td>\n",
       "      <td>0.013674</td>\n",
       "      <td>0.01</td>\n",
       "      <td>{'reg_lambda': 0.01}</td>\n",
       "      <td>-28.838737</td>\n",
       "      <td>-29.187887</td>\n",
       "      <td>-17.164879</td>\n",
       "      <td>-21.380014</td>\n",
       "      <td>-19.370493</td>\n",
       "      <td>-23.188553</td>\n",
       "      <td>4.940724</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>45.311992</td>\n",
       "      <td>0.179354</td>\n",
       "      <td>0.121007</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.1</td>\n",
       "      <td>{'reg_lambda': 0.1}</td>\n",
       "      <td>-28.662182</td>\n",
       "      <td>-29.897873</td>\n",
       "      <td>-17.208915</td>\n",
       "      <td>-21.391101</td>\n",
       "      <td>-19.347181</td>\n",
       "      <td>-23.301607</td>\n",
       "      <td>5.072656</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>45.496202</td>\n",
       "      <td>0.266007</td>\n",
       "      <td>0.121407</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'reg_lambda': 0.5}</td>\n",
       "      <td>-28.596536</td>\n",
       "      <td>-30.042428</td>\n",
       "      <td>-17.217988</td>\n",
       "      <td>-21.205360</td>\n",
       "      <td>-19.225291</td>\n",
       "      <td>-23.257684</td>\n",
       "      <td>5.128168</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45.530604</td>\n",
       "      <td>0.189858</td>\n",
       "      <td>0.122007</td>\n",
       "      <td>0.003688</td>\n",
       "      <td>1</td>\n",
       "      <td>{'reg_lambda': 1}</td>\n",
       "      <td>-28.554756</td>\n",
       "      <td>-29.729489</td>\n",
       "      <td>-17.096022</td>\n",
       "      <td>-21.411857</td>\n",
       "      <td>-19.071609</td>\n",
       "      <td>-23.172903</td>\n",
       "      <td>5.075584</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>45.395796</td>\n",
       "      <td>0.241827</td>\n",
       "      <td>0.122007</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>5</td>\n",
       "      <td>{'reg_lambda': 5}</td>\n",
       "      <td>-28.602481</td>\n",
       "      <td>-29.324453</td>\n",
       "      <td>-17.173026</td>\n",
       "      <td>-21.278032</td>\n",
       "      <td>-19.326938</td>\n",
       "      <td>-23.141138</td>\n",
       "      <td>4.933587</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>45.266989</td>\n",
       "      <td>0.095476</td>\n",
       "      <td>0.120807</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>10</td>\n",
       "      <td>{'reg_lambda': 10}</td>\n",
       "      <td>-28.484249</td>\n",
       "      <td>-29.097556</td>\n",
       "      <td>-17.200880</td>\n",
       "      <td>-21.119169</td>\n",
       "      <td>-19.069553</td>\n",
       "      <td>-22.994437</td>\n",
       "      <td>4.896460</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>45.191985</td>\n",
       "      <td>0.186310</td>\n",
       "      <td>0.123807</td>\n",
       "      <td>0.003060</td>\n",
       "      <td>100</td>\n",
       "      <td>{'reg_lambda': 100}</td>\n",
       "      <td>-28.429836</td>\n",
       "      <td>-27.167255</td>\n",
       "      <td>-17.183463</td>\n",
       "      <td>-20.920282</td>\n",
       "      <td>-19.110286</td>\n",
       "      <td>-22.562361</td>\n",
       "      <td>4.453795</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0      45.700214      0.385310         0.120407        0.002245   \n",
       "1      45.292391      0.243205         0.129207        0.013674   \n",
       "2      45.311992      0.179354         0.121007        0.001095   \n",
       "3      45.496202      0.266007         0.121407        0.002653   \n",
       "4      45.530604      0.189858         0.122007        0.003688   \n",
       "5      45.395796      0.241827         0.122007        0.002608   \n",
       "6      45.266989      0.095476         0.120807        0.002638   \n",
       "7      45.191985      0.186310         0.123807        0.003060   \n",
       "\n",
       "  param_reg_lambda                 params  split0_test_score  \\\n",
       "0            1e-05  {'reg_lambda': 1e-05}         -28.762461   \n",
       "1             0.01   {'reg_lambda': 0.01}         -28.838737   \n",
       "2              0.1    {'reg_lambda': 0.1}         -28.662182   \n",
       "3              0.5    {'reg_lambda': 0.5}         -28.596536   \n",
       "4                1      {'reg_lambda': 1}         -28.554756   \n",
       "5                5      {'reg_lambda': 5}         -28.602481   \n",
       "6               10     {'reg_lambda': 10}         -28.484249   \n",
       "7              100    {'reg_lambda': 100}         -28.429836   \n",
       "\n",
       "   split1_test_score  split2_test_score  split3_test_score  split4_test_score  \\\n",
       "0         -29.740717         -17.221824         -21.384143         -19.311311   \n",
       "1         -29.187887         -17.164879         -21.380014         -19.370493   \n",
       "2         -29.897873         -17.208915         -21.391101         -19.347181   \n",
       "3         -30.042428         -17.217988         -21.205360         -19.225291   \n",
       "4         -29.729489         -17.096022         -21.411857         -19.071609   \n",
       "5         -29.324453         -17.173026         -21.278032         -19.326938   \n",
       "6         -29.097556         -17.200880         -21.119169         -19.069553   \n",
       "7         -27.167255         -17.183463         -20.920282         -19.110286   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  \n",
       "0       -23.284248        5.056653                7  \n",
       "1       -23.188553        4.940724                5  \n",
       "2       -23.301607        5.072656                8  \n",
       "3       -23.257684        5.128168                6  \n",
       "4       -23.172903        5.075584                4  \n",
       "5       -23.141138        4.933587                3  \n",
       "6       -22.994437        4.896460                2  \n",
       "7       -22.562361        4.453795                1  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_test8 = {\n",
    " 'reg_lambda':[1e-5, 1e-2, 0.1, 0.5,1,5,10, 100]\n",
    "}\n",
    "estimator = XGBRegressor(\n",
    "        learning_rate =0.1,\n",
    "        n_estimators=1000,\n",
    "        max_depth=2,\n",
    "        min_child_weight=0.5,\n",
    "        gamma=0,\n",
    "        subsample=0.9,\n",
    "        n_jobs=-1,\n",
    "        colsample_bytree=0.9,\n",
    "        reg_alpha=0.01,\n",
    "        random_state=42)\n",
    "\n",
    "X_train,y_train,X_test,y_test = get_evaluation_splits()\n",
    "model, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, estimator, \n",
    "                                 param_test8)\n",
    "\n",
    "# Root Mean Squared Error\n",
    "print(\"MAE best score: \",-model.best_score_)\n",
    "print(\"MAE regular: \",mean_absolute_error(y_test,pred))\n",
    "print(\"Best params: \",model.best_params_)\n",
    "results = pd.DataFrame(model.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
