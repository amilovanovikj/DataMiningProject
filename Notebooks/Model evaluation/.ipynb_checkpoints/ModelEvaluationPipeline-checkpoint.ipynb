{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import joblib\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ElasticNetCV,LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import featuretools as ft\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import plot_importance\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y, yhat, title=\"Predictions vs Actual\", output_dir=None):\n",
    "    \"\"\"\n",
    "    Plot the predictions against the actual values\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    y : actual (real) values\n",
    "\n",
    "    yhat : predicted values\n",
    "\n",
    "    title : plot title\n",
    "    \"\"\"\n",
    "\n",
    "    fig = plt.figure(figsize=(15, 6))\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('PM10')\n",
    "    plt.plot(y, label=\"actual\", figure=fig)\n",
    "    plt.plot(yhat, label=\"predicted\", figure=fig)\n",
    "    plt.title(title)\n",
    "    fig.legend()\n",
    "\n",
    "    if output_dir != None:\n",
    "        plt.savefig(os.path.join(output_dir, \"{}.png\".format(title)))\n",
    "\n",
    "    plt.close(fig)\n",
    "\n",
    "def train_and_evaluate(name, model, train,final_eval, output_dir, predictors):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model through the scikit-learn pipeline\n",
    "\n",
    "    A pipeline is build using the supplied model as the final step in the pipeline (see make_pipeline)\n",
    "    Then, the input data is transformed accordingly to be able to fit into the pipeline.\n",
    "\n",
    "    The model is saved after the last step under `output_dir`/models with the filename `name`.joblib,\n",
    "    if you wish to load them later and reevaluate/retrain them.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    name : the name of the model\n",
    "\n",
    "    model : a scikit-learn model object\n",
    "\n",
    "    train : train portion of the dataset\n",
    "\n",
    "    final_eval : last 2*X:X hours from the dataset\n",
    "\n",
    "    output_dir : base directory for saving output files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : mean absolute error of the trained model on last X hours\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---\" * 5)\n",
    "    print(\"Running pipeline for {}\".format(name))\n",
    "\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\n",
    "\n",
    "    pipeline = model\n",
    "\n",
    "    X_train, y_train = train.drop(\n",
    "        [\"PM10\"], axis=1)[predictors].values, train[\"PM10\"].values\n",
    "    X_final, y_final = final_eval.drop(\n",
    "        [\"PM10\"], axis=1)[predictors].values, final_eval[\"PM10\"].values\n",
    "    print(\"Fitting pipeline on all \\\"all available data\\\"\")\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    yhat = pipeline.predict(X_final)\n",
    "    yhat_return = np.expm1(yhat)\n",
    "    y_final_return = np.expm1(y_final)\n",
    "    mae = mean_absolute_error(y_final_return, yhat_return)\n",
    "    print(\"MAE: {}\".format(mae))\n",
    "    plot_predictions(y_final_return, yhat_return, title=\"{} - Predicted vs. Actual\".format(name), output_dir=plot_dir)\n",
    "\n",
    "    # save the model\n",
    "    joblib.dump(model, os.path.join(\n",
    "        output_dir, \"models\", \"{}.joblib\".format(name)))\n",
    "\n",
    "    return yhat_return, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple stacking\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "    \n",
    "averaged_models = AveragingModels(models = (ENet, GBoost, lasso,model_xgb,model_lgb))\n",
    "\n",
    "score = mae(averaged_models,predictors)\n",
    "print(\" Averaged base models score: {:.4f} \\n\",score)\n",
    "\n",
    "\n",
    "# STACKING WITH A META MODEL\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_series(path, ts_column=\"time\"):\n",
    "    \"\"\"\n",
    "    Read a time series from a CSV file.\n",
    "\n",
    "    The CSV file must contain a column with either a UNIX timestamp or a datetime\n",
    "    string with any format supported by Pandas. \n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    path : path to CSV file\n",
    "\n",
    "    ts_column : name of the column containing time data, \"time\" by default\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : Pandas Series object with datetime as index\n",
    "    \"\"\"\n",
    "\n",
    "    # read CSV\n",
    "    df = pd.read_csv(path, parse_dates=[ts_column])\n",
    "    # convert timestamps to datetime objects using panda's to_datetime\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column], unit=\"s\")\n",
    "    # set datetime as index (make time series)\n",
    "    df.index = df[ts_column]\n",
    "    # delete original time column\n",
    "    del df[ts_column]\n",
    "\n",
    "    # remove rows with duplicated time if there are any, keep first duplicate row\n",
    "    df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    df.index.name = ts_column\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_directory_tree(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Create the output directory tree structure specified by `tree` in `output_dir`\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    tree : list of paths to create under `output_dir`\n",
    "\n",
    "    output_dir : path to root of output directory tree\n",
    "    \"\"\"\n",
    "\n",
    "    for d in tree:\n",
    "        try:\n",
    "            path = os.path.join(output_dir, d)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                print(\"Path already exists: {}\".format(d))\n",
    "                print(\"Files may be overwritten\")\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def get_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Get all .csv filenames from the specified directory\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    data_dir : path to directory containing .csv files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : list containing dataset filenames\n",
    "    \"\"\"\n",
    "\n",
    "    return [f for f in os.listdir(data_dir) if os.path.isfile(\n",
    "        os.path.join(data_dir, f)) and f.endswith(\".csv\")]\n",
    "\n",
    "\n",
    "def run(input_dir, output_dir, team_name=\"OrganizersTeam\", predict_window=8760):\n",
    "    \"\"\"\n",
    "    Train and evaluate models for each dataset under `input_dir`\n",
    "\n",
    "    This script trains and evaluates 6 models and ensembles them using meta model stacking\n",
    "    1. Lasso\n",
    "    2. ENet\n",
    "    3. KernelRidge\n",
    "    4. GradientBoostingRegressor\n",
    "    5. XGB\n",
    "    6. LGB\n",
    "    7. Meta stacking of previously mentioned models, LGB acting as the meta\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    input_dir : directory containing datasets\n",
    "\n",
    "    output_dir : directory for saving useful output files (models, etc)\n",
    "\n",
    "    predict_window : number of hours needed to predict, default=12\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : a Pandas DataFrame containing the scores for each trained model\n",
    "    \"\"\"\n",
    "\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    sub_dir = os.path.join(output_dir, \"submissions\")\n",
    "    submission_file_name_fmt = \"{}_{}.csv\"\n",
    "\n",
    "    make_directory_tree([\"models\",\"plots\", \"submissions\"], output_dir)\n",
    "\n",
    "    datasets = get_datasets(input_dir)\n",
    "\n",
    "\n",
    "    # create a scores table to keep MAE for each location:model pair\n",
    "    scores = pd.DataFrame(columns=[\"Location\", \"Model\", \"MAE\"])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # load the dataset\n",
    "        df = read_csv_series(os.path.join(input_dir, dataset))\n",
    "        loc = dataset.split(\".\")[0]\n",
    "        print(df.shape[0])\n",
    "        # shift PM10 for `predict_window` hours ahead\n",
    "        df[\"PM10\"] = df[\"PM10\"].shift(-predict_window)\n",
    "\n",
    "        # split dataset into train, test and evaluation by dates\n",
    "        train_len = int((len(df) - (2 * predict_window)))\n",
    "        train= df[:train_len]\n",
    "        final_eval = df[-(2 * predict_window):-predict_window].copy()\n",
    "        \n",
    "        # initialize model, PUT OPTIMIZED MODELS HERE\n",
    "        models = []\n",
    "        lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "        enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "        #krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "        gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                               max_depth=4, max_features='sqrt',\n",
    "                               min_samples_leaf=15, min_samples_split=10, \n",
    "                               loss='huber', random_state =5)\n",
    "        xgboost = xgb.XGBRegressor(learning_rate =0.1, # OTPIMIZED, not to be changed\n",
    "                                   n_estimators=1200,\n",
    "                                   max_depth=2,\n",
    "                                   min_child_weight=0.5,\n",
    "                                   gamma=0,\n",
    "                                   subsample=0.8,\n",
    "                                   n_jobs=-1,\n",
    "                                   colsample_bytree=0.8,\n",
    "                                   reg_alpha=0.01,\n",
    "                                   random_state=42)\n",
    "        models=[lasso,enet,gboost,xgboost]\n",
    "        \n",
    "        lgboost = lgb.LGBMRegressor(objective='regression',num_leaves=5, # THE META MODEL\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "        \n",
    "        stacked_averaged_models = StackingAveragedModels(base_models = models,\n",
    "                                                         meta_model = lgboost)\n",
    "        \n",
    "        models = [lasso,enet,gboost,xgboost,lgboost,stacked_averaged_models]\n",
    "        print(\"Will train a total of {} models\".format(len(models) * 1))\n",
    "\n",
    "        mae_min = 1e10\n",
    "        yhat_sub = []\n",
    "        predictors = [x for x in df.columns if x not in [\"PM10\",\"time\"]]\n",
    "        \n",
    "        for model_name,model in zip(['lasso','enet','gboost','xgboost','lgboost','stacked'],models):\n",
    "            # get predictions and MAE\n",
    "            yhat, mae = train_and_evaluate(\"{} - {}\".format(loc, model_name), model, train,\n",
    "                                           final_eval, output_dir, predictors)\n",
    "\n",
    "            # save the score (MAE) for the model\n",
    "            scores = scores.append(\n",
    "                {\"Location\": loc, \"Model\": model_name, \"MAE\": mae}, ignore_index=True)\n",
    "\n",
    "            # save the better predictions to `yhat_sub`\n",
    "            if mae < mae_min:\n",
    "                mae_min = mae\n",
    "                yhat_sub = yhat\n",
    "\n",
    "        sub_df = pd.DataFrame(yhat_sub, columns=[\"PM10\"])\n",
    "        sub_df.to_csv(os.path.join(\n",
    "            sub_dir, submission_file_name_fmt.format(team_name, loc)))\n",
    "\n",
    "        scores.to_csv(os.path.join(output_dir, \"scores.csv\"))\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(\"Saved models can be found at {}\".format(models_dir))\n",
    "    print(\"Plots can be found at {}\".format(plots_dir))\n",
    "    print(\"Submissions can be found at {}\".format(sub_dir))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29694\n",
      "Will train a total of 6 models\n",
      "---------------\n",
      "Running pipeline for 2016_onward - lasso\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 24.40942719787309\n",
      "---------------\n",
      "Running pipeline for 2016_onward - enet\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 24.411109159904377\n",
      "---------------\n",
      "Running pipeline for 2016_onward - gboost\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 27.615542239183522\n",
      "---------------\n",
      "Running pipeline for 2016_onward - xgboost\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 27.7063595266007\n",
      "---------------\n",
      "Running pipeline for 2016_onward - lgboost\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 25.165018426807755\n",
      "---------------\n",
      "Running pipeline for 2016_onward - stacked\n",
      "Fitting pipeline on all \"all available data\"\n",
      "MAE: 29.094314530296145\n",
      "Done\n",
      "Saved models can be found at pm10_scores\\models\n",
      "Plots can be found at pm10_scores\\plots\n",
      "Submissions can be found at pm10_scores\\submissions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Location</th>\n",
       "      <th>Model</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>lasso</td>\n",
       "      <td>24.409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>enet</td>\n",
       "      <td>24.411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>gboost</td>\n",
       "      <td>27.616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>27.706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>lgboost</td>\n",
       "      <td>25.165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2016_onward</td>\n",
       "      <td>stacked</td>\n",
       "      <td>29.094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Location    Model    MAE\n",
       "0  2016_onward    lasso 24.409\n",
       "1  2016_onward     enet 24.411\n",
       "2  2016_onward   gboost 27.616\n",
       "3  2016_onward  xgboost 27.706\n",
       "4  2016_onward  lgboost 25.165\n",
       "5  2016_onward  stacked 29.094"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = 'pm10_scores'\n",
    "run(\"data_pm10/preprocessed\", output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXAMPLE ENSEMBLING (BOOSTING->STACKING)\n",
    "# lgb_pred = np.expm1(model_lgb.predict(X_final[predictors]))\n",
    "# xgb_pred = np.expm1(model_xgb.predict(X_final[predictors]))\n",
    "# stacked_pred = np.expm1(stacked_averaged_models.predict(X_final[predictors].values))\n",
    "\n",
    "# best_score = 10000\n",
    "\n",
    "# def fit_ensemble(best_score=best_score):\n",
    "#     params=[]\n",
    "#     for i in np.arange(0,1.1,0.05):\n",
    "#         for j in np.arange(0,1.1,0.05):\n",
    "#             for k in np.arange(0,1.1,0.05):\n",
    "#                 if(i+j+k==1):\n",
    "#                     pred = lgb_pred*i+stacked_pred*j+xgb_pred*k\n",
    "#                     score = mean_absolute_error(true_pred,pred)\n",
    "#                     if(score<best_score and i>=0.1 and j>=0.1 and k>=0.1):\n",
    "#                         best_score=score\n",
    "#                         params = [i,j,k]\n",
    "#     return params\n",
    "\n",
    "# best_params = fit_ensemble()\n",
    "# ensemble = lgb_pred*best_params[0]+stacked_pred*best_params[1]+xgb_pred*best_params[2] # CRAETING AN ENSEMBLE\n",
    "# print(mean_absolute_error(true_pred,ensemble))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
