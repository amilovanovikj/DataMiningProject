{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import warnings\n",
    "import featuretools as ft\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy.special import boxcox1p\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_series(path, ts_column=\"time\"):\n",
    "    \"\"\"\n",
    "    Read a time series from a CSV file.\n",
    "\n",
    "    The CSV file must contain a column with either a UNIX timestamp or a datetime\n",
    "    string with any format supported by Pandas. \n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    path : path to CSV file\n",
    "\n",
    "    ts_column : name of the column containing time data, \"time\" by default\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : Pandas Series object with datetime as index\n",
    "    \"\"\"\n",
    "\n",
    "    # read CSV\n",
    "    df = pd.read_csv(path, parse_dates=[ts_column])\n",
    "    # convert timestamps to datetime objects using panda's to_datetime\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column], unit=\"s\")\n",
    "    # set datetime as index (make time series)\n",
    "    df.index = df[ts_column]\n",
    "    # delete original time column\n",
    "    del df[ts_column]\n",
    "\n",
    "    # remove rows with duplicated time if there are any, keep first duplicate row\n",
    "    df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    df.index.name = ts_column\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def describe_series(df):\n",
    "    \"\"\"\n",
    "    Show basic information about a Pandas Series or DataFrame\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    df : a Pandas Series or DataFrame object\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Head:\")\n",
    "    print(df.head())\n",
    "    print(\"Stats:\")\n",
    "    print(df.describe())\n",
    "    print(\"Count:\")\n",
    "    print(df.count())\n",
    "    print(\"Columns: {}\".format(df.columns))\n",
    "\n",
    "    print(\"Start of time: {}\".format(str(df.index[0])))\n",
    "    print(\"End of time: {}\".format(str(df.index[-1])))\n",
    "\n",
    "\n",
    "def make_directory_tree(tree, output_dir):\n",
    "    \"\"\"\n",
    "    Create the output directory tree structure specified by `tree` in `output_dir`\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    tree : list of paths to create under `output_dir`\n",
    "\n",
    "    output_dir : path to root of output directory tree\n",
    "    \"\"\"\n",
    "\n",
    "    for d in tree:\n",
    "        try:\n",
    "            path = os.path.join(output_dir, d)\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        except OSError as e:\n",
    "            if e.errno == errno.EEXIST and os.path.isdir(path):\n",
    "                print(\"Path already exists: {}\".format(d))\n",
    "                print(\"Files may be overwritten\")\n",
    "                continue\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "\n",
    "def get_datasets(data_dir):\n",
    "    \"\"\"\n",
    "    Get all .csv filenames from the specified directory\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    data_dir : path to directory containing .csv files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : list containing dataset filenames\n",
    "    \"\"\"\n",
    "\n",
    "    return [f for f in os.listdir(data_dir) if os.path.isfile(\n",
    "        os.path.join(data_dir, f)) and f.endswith(\".csv\")]\n",
    "\n",
    "def preprocess_report(path, outpath):\n",
    "    \"\"\"\n",
    "    Preprocess a report\n",
    "    \n",
    "    Parameters\n",
    "    -----\n",
    "    path : path to input dataset\n",
    "\n",
    "    outpath : path at which to save the processed dataset\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : processed pollution series\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Processing {}\".format(path))\n",
    "\n",
    "    # read CSV\n",
    "    df = pd.read_csv(path, parse_dates=['time'])\n",
    "    # convert timestamps to datetime objects using panda's to_datetime\n",
    "    # Baseline pollution preprocess\n",
    "\n",
    "    # convert series values to numeric type (np.float64 by default)\n",
    "    # place NaN values in rows with invalid format\n",
    "\n",
    "    objects = [col for col in df.columns if df[col].dtype == \"object\"]\n",
    "    objects.remove('time')\n",
    "    \n",
    "    for col in objects:\n",
    "        for row_no in range(df.shape[0]):\n",
    "            if not pd.isnull(df.loc[row_no, col]):\n",
    "                df.loc[row_no, col] = rreplace(df.loc[row_no, col])\n",
    "                \n",
    "    # if data is missing for more than 1 hour or > 1000, remove those rows\n",
    "    # also set negative measurements to np.nan\n",
    "    \n",
    "    for col in objects:\n",
    "        df[col]= pd.to_numeric(df[col],errors='coerce')\n",
    "    \n",
    "    weather_cat_cols=[\"precipType\",\"summary\"]\n",
    "    \n",
    "    #Baseline weather preprocess\n",
    "    df = df[1:].drop([\"icon\"], axis=1)\n",
    "    df = df.fillna({\"precipType\": \"no precip\",\n",
    "                    \"precipAccumulation\": 0, \"cloudCover\": 0})\n",
    "    df[\"pressure\"].interpolate(inplace=True, limit=3)\n",
    "\n",
    "    # check for columns with discrete values, one-hot encode them\n",
    "    categorical_column_names = weather_cat_cols\n",
    "    # cannot one-hot encode NaN values\n",
    "    for column_name in categorical_column_names:\n",
    "        one_hot = pd.get_dummies(df[column_name])\n",
    "        df = df.join(one_hot)\n",
    "\n",
    "    df = df.drop(categorical_column_names, axis=1)\n",
    "    \n",
    "    \n",
    "    ts_column = 'time'\n",
    "    df[ts_column] = pd.to_datetime(df[ts_column], unit=\"s\")\n",
    "    # set datetime as index (make time series)\n",
    "    df.index = df[ts_column]\n",
    "    # delete original time column\n",
    "    del df[ts_column]\n",
    "\n",
    "    # remove rows with duplicated time if there are any, keep first duplicate row\n",
    "    df = df.loc[~df.index.duplicated(keep=\"first\")]\n",
    "\n",
    "    df.index.name = ts_column\n",
    "    \n",
    "    # create a new datetime index with hourly frequency\n",
    "    idx = pd.date_range(start=df.index.min(), end=df.index.max(), freq='H')\n",
    "    idx_name = df.index.name\n",
    "    \n",
    "    # reindex our series\n",
    "    df = df.reindex(idx)\n",
    "    df.index.name = idx_name\n",
    "    \n",
    "   \n",
    "    # otherwise, interpolate\n",
    "    df['PM10'].loc[(df['PM10'] > 1000) | (df['PM10'] < 0)] = np.nan\n",
    "    df['PM10'] = df['PM10'].interpolate(method=\"linear\", limit=1, limit_area=\"inside\")\n",
    "    df = df[df['PM10'].notna()]\n",
    "\n",
    "    objects = [col for col in df.columns if df[col].dtype==\"object\"]\n",
    "    objects.remove('time')\n",
    "    \n",
    "    target_col = \"PM10\"\n",
    "    df[target_col] = np.log1p(df[target_col]) #Logarithming here\n",
    "    target = df[target_col] #Logarithmed target var\n",
    "    df['windBearing'].fillna(0,inplace=True) #according to dark sky api\n",
    "    \n",
    "    #Dropping apparent temperature\n",
    "    df.drop(\"apparentTemperature\",axis=1,inplace=True) #high correlation\n",
    "    \n",
    "    #Outlier detection and setting to nan to interpolate later\n",
    "    \n",
    "    columns = [\"AQI\",\"CO\",\"CO2\",\"NO2\",\"O3\",\"PM25\",\"SO2\",\"cloudCover\",\"dewPoint\",\"humidity\",\n",
    "              \"ozone\",\"precipAccumulation\",\"precipIntensity\",\"precipProbability\",\"pressure\",\"temperature\",\"uvIndex\",\n",
    "              \"visibility\",\"windBearing\",\"windGust\",\"windSpeed\"]\n",
    "    \n",
    "    for column in columns:\n",
    "        pHigh = np.percentile(a, 98)\n",
    "        pLow = np.percentile(a,2)\n",
    "        df[column].loc[df[column] <pLow  or df[column] > pHigh]=np.nan\n",
    "    \n",
    "    #Missing data analysis\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data = missing_data[missing_data['Total']>0]\n",
    "    \n",
    "    # Dropping all features with missing data > 50%\n",
    "    missing_data_50_cols = list(missing_data[missing_data['Percent']>0.5].index.values)\n",
    "    df.drop(columns=missing_data_50_cols,axis=1,inplace=True)\n",
    "    \n",
    "    #Dropping rows with >75% of missing data\n",
    "    thresh = int((len(df.columns)*75)/100)\n",
    "    df.dropna(thresh=thresh,inplace=True)\n",
    "    \n",
    "    #Removing cols which were discarded\n",
    "    weather_cols = list(set(weather_cols)-set(missing_data_50_cols))\n",
    "    pollution_cols = list(set(pollution_cols)-set(missing_data_50_cols))\n",
    "    \n",
    "    #Imputing with interpolation\n",
    "    df.index = pd.DatetimeIndex(df['time'])\n",
    "    \n",
    "    for weather_col in weather_cols:\n",
    "        df[weather_col].interpolate(inplace=True, limit=4,method='time',limit_direction='both') # adapt limit here\n",
    "        \n",
    "    for pollution_col in pollution_cols:\n",
    "        df[pollution_col].interpolate(inplace=True, limit=8,method='time',limit_direction='both') # adapt limit here\n",
    "    \n",
    "    #Next impute other missing values with aggregating functions, such as getting the median of the same day in the \n",
    "    #same month of each year\n",
    "    unique_days = list(set(list(map(lambda x: x[5:10], df['time'].values))))\n",
    "    pat = '|'.join(unique_hours)\n",
    "    s = df['time'].str.extract('('+ pat + ')', expand=False)\n",
    "    \n",
    "    def transform_func(row,col,median):\n",
    "        if np.isnan(row[col]):\n",
    "            row[col] = median[row['time'][5:10]]\n",
    "        return row\n",
    "    \n",
    "    for weather_col in weather_cols:\n",
    "        median_values = df.groupby(s)[weather_col].median()\n",
    "        df = df.apply(transform_func,axis=1,col=weather_col,median=median_values)\n",
    "    \n",
    "    #Next drop AQI, because we will generate new ones \n",
    "    df.drop(['AQI'], axis=1,inplace=True)\n",
    "    \n",
    "    #Impute pollution features with aggregating functions as above\n",
    "    for pollution_col in pollution_cols:\n",
    "        median_values = df.groupby(s)[pollution_col].median()\n",
    "        df = df.apply(transform_func,axis=1,col=pollution_col,median=median_values)\n",
    "        \n",
    "    #Finish imputation\n",
    "    total = df.isnull().sum().sort_values(ascending=False)\n",
    "    percent = (df.isnull().sum()/df.isnull().count()).sort_values(ascending=False)\n",
    "    missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "    missing_data_cols = list(missing_data[missing_data['Total']>0].index.values)\n",
    "    weather_cols = list(set(weather_cols).intersection(set(missing_data_cols)))\n",
    "    pollution_cols = list(set(pollution_cols).intersection(set(missing_data_cols)))\n",
    "    \n",
    "    for weather_col in weather_cols:\n",
    "        df[weather_col].interpolate(inplace=True, limit=4,method='time',limit_direction='both') # adapt limit here\n",
    "        \n",
    "    for pollution_col in pollution_cols:\n",
    "        df[pollution_col].interpolate(inplace=True, limit=8,method='time',limit_direction='both') # adapt limit here\n",
    "    \n",
    "    for missing_col in missing_data_cols:\n",
    "        df[missing_col].fillna(df[missing_col].median(),inplace=True)\n",
    "    \n",
    "    # FEATURE GENERATION\n",
    "    \n",
    "    def mapFunckijaPM25(row):\n",
    "        row['AQI_PM25'] = df[row.name - datetime.timedelta(days=1) : row.name]['PM25'].mean()\n",
    "        return row\n",
    "    df = df.apply(mapFunckijaPM25,axis=1)\n",
    "\n",
    "    def mapFunckijaPM10(row):\n",
    "        row['AQI_PM10'] = df[row.name - datetime.timedelta(days=1) : row.name]['PM10'].mean()\n",
    "        return row\n",
    "    df = df.apply(mapFunckijaPM10,axis=1)\n",
    "\n",
    "    def mapFunckijaO3(row):\n",
    "        row['AQI_O3'] = df[row.name - datetime.timedelta(days=1) : row.name]['O3'].mean()\n",
    "        return row\n",
    "    #return train['PM25'][datetime.datetime[]]\n",
    "    df = df.apply(mapFunckijaO3,axis=1)\n",
    "    \n",
    "    def mapFunkcijaPM10history(row):\n",
    "        row['PM10_history'] = df[row.name-datetime.timedelta(hours=3) : row.name-datetime.timedelta(hours=1)]['PM10'].mean()\n",
    "        return row\n",
    "    df = df.apply(mapFunkcijaPM10history,axis=1)\n",
    "\n",
    "    target_cols = ['O3', 'PM10', 'PM25', 'cloudCover', 'dewPoint', 'humidity',\n",
    "       'precipAccumulation', 'precipIntensity', 'precipProbability',\n",
    "       'pressure', 'temperature', 'uvIndex', 'visibility', 'windBearing',\n",
    "       'windSpeed','AQI_PM25', 'AQI_PM10', 'AQI_O3','PM10_history']\n",
    "    def mapFunkcijaDelta(row):\n",
    "        for col in target_cols:\n",
    "            row[col+\"_delta\"] = df[row.name:row.name][col] - df[row.name-datetime.timedelta(hours=1)\n",
    "                                                                     :row.name-datetime.timedelta(hours=1)][col]\n",
    "        return row\n",
    "    df.iloc[1:,:] = df.iloc[1:,:].apply(mapFunkcijaDelta,axis=1)\n",
    "    \n",
    "    def mapFunkcijaMesecAndWeekNum(row):\n",
    "        row.name = pd.Timestamp(row.name) #converting to timestamp\n",
    "        row['month'] = row.name.month\n",
    "        row['week_no'] = row.name.week            \n",
    "        return row\n",
    "    df = df.apply(mapFunkcijaMesec,axis=1)\n",
    "    \n",
    "    # SKEW ANALYSIS AND BOX COX TRANSFORMATION HERE\n",
    "    numeric_feats = dt.dtypes[dt.dtypes != \"object\"].index\n",
    "\n",
    "    # Check the skew of all numerical features\n",
    "    skewed_feats = df[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n",
    "    skewness = pd.DataFrame({'Skew' :skewed_feats})\n",
    "    skewness = skewness[(skewness.Skew) > 0.75] #define thresh here, cv later\n",
    "    skewed_features = skewness.index\n",
    "    lam = 0.15\n",
    "    for feat in skewed_features:\n",
    "        df[feat] = boxcox1p(df[feat], lam)\n",
    "\n",
    "    return df\n",
    "\n",
    "def run_preprocessing(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Iterate over the input datasets and process them, generating new datasets in output directory.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    input_dir : path to directory containing raw data\n",
    "\n",
    "    output_dir : output directory for storing processed data\n",
    "    \"\"\"\n",
    "\n",
    "    make_directory_tree([\"preprocessed\"], output_dir)\n",
    "    reports_dir = os.path.join(input_dir, \"data\")\n",
    "\n",
    "    sorted_reports = sorted(get_datasets(reports_dir))\n",
    "\n",
    "    for report in sorted_reports:\n",
    "        loc = report.split('.')[0].split('_')[-1]\n",
    "        print(\"Processing reports for {}\".format(loc))\n",
    "        df_report = preprocess_report(os.path.join(reports_dir, report), os.path.join(\n",
    "            output_dir, \"preprocessed\", report))\n",
    "        \n",
    "        # drop columns with NaN for PM10\n",
    "        df = df.dropna(subset=[\"PM10\"])\n",
    "\n",
    "        # save combined dataset\n",
    "        pd.DataFrame(df).to_csv(os.path.join(\n",
    "            output_dir, \"preprocessedCSV\", report))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_preprocessing(\"../data/raw\", \"../data/processed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
