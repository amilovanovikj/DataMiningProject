{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ElasticNetCV,LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import featuretools as ft\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import plot_importance\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(name, model, train, test, evaluation, final_eval, output_dir, predictors):\n",
    "    \"\"\"\n",
    "    Train and evaluate a model through the scikit-learn pipeline\n",
    "\n",
    "    A pipeline is build using the supplied model as the final step in the pipeline (see make_pipeline)\n",
    "    Then, the input data is transformed accordingly to be able to fit into the pipeline.\n",
    "\n",
    "    The model is saved after the last step under `output_dir`/models with the filename `name`.joblib,\n",
    "    if you wish to load them later and reevaluate/retrain them.\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    name : the name of the model\n",
    "\n",
    "    model : a scikit-learn model object\n",
    "\n",
    "    train : train portion of the dataset\n",
    "\n",
    "    test : test portion of the dataset\n",
    "\n",
    "    evaluation : evaluation portion of the dataset\n",
    "\n",
    "    final_eval : last X hours from the dataset\n",
    "\n",
    "    output_dir : base directory for saving output files\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : mean absolute error of the trained model on last X hours\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---\" * 5)\n",
    "    print(\"Running pipeline for {}\".format(name))\n",
    "\n",
    "    plot_dir = os.path.join(output_dir, \"plots\")\n",
    "\n",
    "    pipeline = model\n",
    "\n",
    "    X_train, y_train = train.drop(\n",
    "        [\"PM10\"], axis=1).values, train[\"PM10\"].values\n",
    "    X_test, y_test = test.drop([\"PM10\"], axis=1).values, test[\"PM10\"].values\n",
    "    X_eval, y_eval = evaluation.drop(\n",
    "        [\"PM10\"], axis=1).values, evaluation[\"PM10\"].values\n",
    "    X_final, y_final = final_eval.drop(\n",
    "        [\"PM10\"], axis=1), final_eval[\"PM10\"].values\n",
    "    X_train = np.concatenate([X_train, X_test, X_eval])\n",
    "    y_train = np.concatenate([y_train, y_test, y_eval])\n",
    "    print(\"Fitting pipeline on all \\\"all available data\\\"\")\n",
    "    pipeline.fit(X_train[predictors].values, y_train[predictors].values)\n",
    "    yhat = pipeline.predict(X_final[predictors].values)\n",
    "    yhat = np.expm1(yhat)\n",
    "    y_final = np.expm1(y_final)\n",
    "    mae = mean_absolute_error(y_final, yhat)\n",
    "    print(\"MAE: {}\".format(mae))\n",
    "    plot_predictions(\n",
    "        y_final, yhat, title=\"{} - Predicted vs. Actual\".format(name), output_dir=plot_dir)\n",
    "\n",
    "    # save the model\n",
    "    joblib.dump(model, os.path.join(\n",
    "        output_dir, \"models\", \"{}.joblib\".format(name)))\n",
    "\n",
    "    return yhat, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STACKING WITH A META MODEL\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(input_dir, output_dir, team_name=\"OrganizersTeam\", predict_window=12):\n",
    "    \"\"\"\n",
    "    Train and evaluate models for each dataset under `input_dir`\n",
    "\n",
    "    This script trains and evaluates 6 models and ensembles them using meta model stacking\n",
    "    1. Lasso\n",
    "    2. ENet\n",
    "    3. KernelRidge\n",
    "    4. GradientBoostingRegressor\n",
    "    5. XGB\n",
    "    6. LGB\n",
    "    7. Meta stacking of previously mentioned models, LGB acting as the meta\n",
    "\n",
    "    Parameters\n",
    "    -----\n",
    "    input_dir : directory containing datasets\n",
    "\n",
    "    output_dir : directory for saving useful output files (models, etc)\n",
    "\n",
    "    predict_window : number of hours needed to predict, default=12\n",
    "\n",
    "    Returns\n",
    "    -----\n",
    "    ret : a Pandas DataFrame containing the scores for each trained model\n",
    "    \"\"\"\n",
    "\n",
    "    models_dir = os.path.join(output_dir, \"models\")\n",
    "    plots_dir = os.path.join(output_dir, \"plots\")\n",
    "    sub_dir = os.path.join(output_dir, \"submissions\")\n",
    "    submission_file_name_fmt = \"{}_{}.csv\"\n",
    "\n",
    "    make_directory_tree([\"models\",\"plots\", \"submissions\"], output_dir)\n",
    "\n",
    "    datasets = get_datasets(input_dir)\n",
    "\n",
    "    print(\"Will train a total of {} models\".format(len(datasets) * 1))\n",
    "\n",
    "    # create a scores table to keep MAE for each location:model pair\n",
    "    scores = pd.DataFrame(columns=[\"Location\", \"Model\", \"MAE\"])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # load the dataset\n",
    "        df = read_csv_series(os.path.join(input_dir, dataset))\n",
    "        loc = dataset.split(\".\")[0]\n",
    "\n",
    "        # shift PM10 for `predict_window` hours ahead\n",
    "        df[\"PM10\"] = df[\"PM10\"].shift(-predict_window)\n",
    "\n",
    "        # split dataset into train, test and evaluation by dates\n",
    "        # additionally, leave the last 48 hours for final evaluation\n",
    "        train_len = int(len(df) * 0.65) - (2 * predict_window)\n",
    "        test_len = int(len(df) * 0.25) - (2 * predict_window)\n",
    "        eval_len = len(df) - train_len - test_len - (2 * predict_window)\n",
    "        train, test, evaluation = df[:train_len], df[train_len:train_len +\n",
    "                                                     test_len], df[train_len+test_len:train_len+test_len+eval_len]\n",
    "        final_eval = df[-(2 * predict_window):-predict_window].copy()\n",
    "\n",
    "        # initialize model, PUT OPTIMIZED MODELS HERE\n",
    "        models = [\n",
    "            lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1)),\n",
    "            enet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3)),\n",
    "            krr = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5),\n",
    "            gboost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5),\n",
    "            xgboost = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)]\n",
    "        \n",
    "        lgboost = lgb.LGBMRegressor(objective='regression',num_leaves=5, # THE META MODEL\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n",
    "        \n",
    "        stacked_averaged_models = StackingAveragedModels(base_models = models,\n",
    "                                                         meta_model = lgboost)\n",
    "        \n",
    "        models = [stacked_averaged_models]\n",
    "        \n",
    "\n",
    "        mae_min = 1e10\n",
    "        yhat_sub = []\n",
    "        predictors = [x for x in df.columns if x not in [\"PM10\",\"time\"]]\n",
    "        \n",
    "        for model in models:\n",
    "            # get predictions and MAE\n",
    "            yhat, mae = train_and_evaluate(\"{} - {}\".format(loc, model[0]), model[1], train,\n",
    "                                           test, evaluation, final_eval, output_dir, predictors)\n",
    "\n",
    "            # save the score (MAE) for the model\n",
    "            scores = scores.append(\n",
    "                {\"Location\": loc, \"Model\": model[0], \"MAE\": mae}, ignore_index=True)\n",
    "\n",
    "            # save the better predictions to `yhat_sub`\n",
    "            if mae < mae_min:\n",
    "                mae_min = mae\n",
    "                yhat_sub = yhat\n",
    "\n",
    "        sub_df = pd.DataFrame(yhat_sub, columns=[\"PM10\"])\n",
    "        sub_df.to_csv(os.path.join(\n",
    "            sub_dir, submission_file_name_fmt.format(team_name, loc)))\n",
    "\n",
    "    scores.to_csv(os.path.join(output_dir, \"scores.csv\"))\n",
    "\n",
    "    print(\"Done\")\n",
    "    print(\"Saved models can be found at {}\".format(models_dir))\n",
    "    print(\"Plots can be found at {}\".format(plots_dir))\n",
    "    print(\"Submissions can be found at {}\".format(sub_dir))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run(\"../data/preprocessed\", output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
