{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "import warnings\n",
    "from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC,ElasticNetCV,LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import featuretools as ft\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from xgboost import plot_importance\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from scipy.special import boxcox1p\n",
    "from sklearn.metrics import r2_score\n",
    "import lightgbm as lgb\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "import warnings\n",
    "def ignore_warn(*args, **kwargs):\n",
    "    pass\n",
    "warnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"centar_preprocessed.csv\")\n",
    "#print(\"Centar shape: \",train['PM10'].head(24))\n",
    "# print(train.dtypes)\n",
    "\n",
    "# total = train.isnull().sum().sort_values(ascending=False)\n",
    "# percent = (train.isnull().sum()/train.isnull().count()).sort_values(ascending=False)\n",
    "# missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "# missing_data = missing_data[missing_data['Total']>0]\n",
    "# missing_data\n",
    "# print(train.head())\n",
    "\n",
    "# number of hours we need to predict ahead\n",
    "predict_window = 12\n",
    "\n",
    "# shift PM10 for `predict_window` hours ahead\n",
    "df[\"PM10\"] = df[\"PM10\"].shift(-predict_window)\n",
    "\n",
    "\n",
    "# train.tail(36)\n",
    "#print(\"Centar shape: \",train['PM10'].head(12))\n",
    "\n",
    "train_len = int(len(df) * 0.65) - (2 * predict_window)\n",
    "test_len = int(len(df) * 0.25) - (2 * predict_window)\n",
    "eval_len = len(df) - train_len - test_len - (2 * predict_window)\n",
    "total_len = train_len+test_len+eval_len\n",
    "train = df[:total_len]\n",
    "final_eval = df[-(2 * predict_window):-predict_window]\n",
    "\n",
    "X_train, y_train = train.drop(\n",
    "        [\"PM10\",\"time\"], axis=1), train[\"PM10\"]\n",
    "X_final, y_final = final_eval.drop(\n",
    "    [\"PM10\",\"time\"], axis=1), final_eval[\"PM10\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_folds = 10 \n",
    "\n",
    "def mae(model,predictors):\n",
    "    model.fit(X_train[predictors],y_train)\n",
    "    predictions = model.predict(X_final[predictors])\n",
    "    normal_pred = np.expm1(predictions)\n",
    "    true_pred = np.expm1(y_final)\n",
    "    mae = mean_absolute_error(true_pred, normal_pred)\n",
    "    return(mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\n",
    "ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\n",
    "KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n",
    "GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =5)\n",
    "model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n",
    "                             learning_rate=0.05, max_depth=3, \n",
    "                             min_child_weight=1.7817, n_estimators=2200,\n",
    "                             reg_alpha=0.4640, reg_lambda=0.8571,\n",
    "                             subsample=0.5213, silent=1,\n",
    "                             random_state =7, nthread = -1)\n",
    "model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n",
    "                              learning_rate=0.05, n_estimators=720,\n",
    "                              max_bin = 55, bagging_fraction = 0.8,\n",
    "                              bagging_freq = 5, feature_fraction = 0.2319,\n",
    "                              feature_fraction_seed=9, bagging_seed=9,\n",
    "                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lasso score: 10.4436\n",
      "\n",
      "ElasticNet score: 10.4183\n",
      "\n",
      "Gradient Boosting score: 6.8224\n",
      "\n",
      "XGB score: 6.1274\n",
      "\n",
      "LGB score: 5.9879\n",
      "\n",
      "5.9878832376697675\n"
     ]
    }
   ],
   "source": [
    "predictors = [x for x in df.columns if x not in [\"PM10\",\"time\"]]\n",
    "score = mae(lasso,predictors)\n",
    "print(\"\\nLasso score: {:.4f}\\n\".format(score))\n",
    "score = mae(ENet,predictors)\n",
    "print(\"ElasticNet score: {:.4f}\\n\".format(score))\n",
    "# score = mae(KRR)\n",
    "# print(\"Kernel Ridge score: {:.4f}\\n\".format(score))\n",
    "score = mae(GBoost,predictors)\n",
    "print(\"Gradient Boosting score: {:.4f}\\n\".format(score))\n",
    "\n",
    "# def xgboost_model(alg,predictors):\n",
    "#     #Fit the algorithm on the data\n",
    "#     alg.fit(X_train[predictors],y_train)\n",
    "        \n",
    "#     #Predict val set:\n",
    "#     dvalid_predictions = alg.predict(X_final[predictors])  \n",
    "#     normal_pred = np.expm1(dvalid_predictions)\n",
    "#     true_pred = np.expm1(y_final)\n",
    "#     #Print model report:\n",
    "#     print (\"\\nModel Report for XGB\")\n",
    "#     print (\"Mean absolute error : \",mean_absolute_error(true_pred, normal_pred))                \n",
    "#     feat_imp = pd.Series(alg.get_booster().get_fscore()).sort_values(ascending=False)\n",
    "#     feat_imp.plot(kind='bar', title='Feature Importances')\n",
    "#     plt.ylabel('Feature Importance Score')\n",
    "    \n",
    "score = mae(model_xgb,predictors)\n",
    "print(\"XGB score: {:.4f}\\n\".format(score))  \n",
    "\n",
    "score = mae(model_lgb,predictors)\n",
    "print(\"LGB score: {:.4f}\\n\".format(score))  \n",
    "\n",
    "score = mae(model_lgb,predictors)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Averaged base models score: {:.4f} \n",
      " 7.438248162289586\n"
     ]
    }
   ],
   "source": [
    "#Simple stacking\n",
    "class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, models):\n",
    "        self.models = models\n",
    "        \n",
    "    # we define clones of the original models to fit the data in\n",
    "    def fit(self, X, y):\n",
    "        self.models_ = [clone(x) for x in self.models]\n",
    "        \n",
    "        # Train cloned base models\n",
    "        for model in self.models_:\n",
    "            model.fit(X, y)\n",
    "\n",
    "        return self\n",
    "    \n",
    "    #Now we do the predictions for cloned models and average them\n",
    "    def predict(self, X):\n",
    "        predictions = np.column_stack([\n",
    "            model.predict(X) for model in self.models_\n",
    "        ])\n",
    "        return np.mean(predictions, axis=1)  \n",
    "    \n",
    "averaged_models = AveragingModels(models = (ENet, GBoost, lasso,model_xgb,model_lgb))\n",
    "\n",
    "score = mae(averaged_models,predictors)\n",
    "print(\" Averaged base models score: {:.4f} \\n\",score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Averaged models score: {:.4f} \n",
      " 8.060458803957731\n"
     ]
    }
   ],
   "source": [
    "# STACKING WITH A META MODEL\n",
    "\n",
    "class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n",
    "    def __init__(self, base_models, meta_model, n_folds=5):\n",
    "        self.base_models = base_models\n",
    "        self.meta_model = meta_model\n",
    "        self.n_folds = n_folds\n",
    "   \n",
    "    # We again fit the data on clones of the original models\n",
    "    def fit(self, X, y):\n",
    "        self.base_models_ = [list() for x in self.base_models]\n",
    "        self.meta_model_ = clone(self.meta_model)\n",
    "        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n",
    "        \n",
    "        # Train cloned base models then create out-of-fold predictions\n",
    "        # that are needed to train the cloned meta-model\n",
    "        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        for i, model in enumerate(self.base_models):\n",
    "            for train_index, holdout_index in kfold.split(X, y):\n",
    "                instance = clone(model)\n",
    "                self.base_models_[i].append(instance)\n",
    "                instance.fit(X[train_index], y[train_index])\n",
    "                y_pred = instance.predict(X[holdout_index])\n",
    "                out_of_fold_predictions[holdout_index, i] = y_pred\n",
    "                \n",
    "        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n",
    "        self.meta_model_.fit(out_of_fold_predictions, y)\n",
    "        return self\n",
    "   \n",
    "    #Do the predictions of all base models on the test data and use the averaged predictions as \n",
    "    #meta-features for the final prediction which is done by the meta-model\n",
    "    def predict(self, X):\n",
    "        meta_features = np.column_stack([\n",
    "            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n",
    "            for base_models in self.base_models_ ])\n",
    "        return self.meta_model_.predict(meta_features)\n",
    "    \n",
    "stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost,lasso,model_xgb),\n",
    "                                                 meta_model = model_lgb)\n",
    "\n",
    "stacked_averaged_models.fit(X_train[predictors].values,y_train.values)\n",
    "predictions = stacked_averaged_models.predict(X_final[predictors].values)\n",
    "normal_pred = np.expm1(predictions)\n",
    "true_pred = np.expm1(y_final)\n",
    "\n",
    "mae = mean_absolute_error(true_pred, normal_pred)\n",
    "print(\"Stacking Averaged models score: {:.4f} \\n\",mae)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.121097637613624\n"
     ]
    }
   ],
   "source": [
    "lgb_pred = np.expm1(model_lgb.predict(X_final[predictors]))\n",
    "xgb_pred = np.expm1(model_xgb.predict(X_final[predictors]))\n",
    "stacked_pred = np.expm1(stacked_averaged_models.predict(X_final[predictors].values))\n",
    "\n",
    "best_score = 10000\n",
    "\n",
    "def fit_ensemble(best_score=best_score):\n",
    "    params=[]\n",
    "    for i in np.arange(0,1.1,0.05):\n",
    "        for j in np.arange(0,1.1,0.05):\n",
    "            for k in np.arange(0,1.1,0.05):\n",
    "                if(i+j+k==1):\n",
    "                    pred = lgb_pred*i+stacked_pred*j+xgb_pred*k\n",
    "                    score = mean_absolute_error(true_pred,pred)\n",
    "                    if(score<best_score and i>=0.1 and j>=0.1 and k>=0.1):\n",
    "                        best_score=score\n",
    "                        params = [i,j,k]\n",
    "    return params\n",
    "\n",
    "best_params = fit_ensemble()\n",
    "ensemble = lgb_pred*best_params[0]+stacked_pred*best_params[1]+xgb_pred*best_params[2] # CRAETING AN ENSEMBLE\n",
    "print(mean_absolute_error(true_pred,ensemble))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
